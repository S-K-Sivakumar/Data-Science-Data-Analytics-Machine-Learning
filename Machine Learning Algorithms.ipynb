{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Fuctions Across Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_unique_values_in_cols(df,cols = None): \n",
    "    if type(cols) == str:\n",
    "        cols = [cols]\n",
    "    elif cols == None:\n",
    "        cols = list(df.columns)\n",
    "    \n",
    "    d_unique = {}\n",
    "    for col in cols:\n",
    "        d_unique[col] = df[col].value_counts().count()\n",
    "    return d_unique\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df_in,outlier_cols):\n",
    "    df = df_in.copy()\n",
    "    if type(outlier_cols) == str:\n",
    "        outlier_cols = [outlier_cols]\n",
    "    \n",
    "    desc = df.describe()\n",
    "    for col in outlier_cols:\n",
    "        q1 = desc.loc['25%',col]\n",
    "        q3 = desc.loc['75%',col]\n",
    "        iqr = q3 - q1\n",
    "        low = q1 - (1.5 * iqr)\n",
    "        high = q3 + (1.5 * iqr)\n",
    "        df = df[(df[col] >= low) & (df[col] <= high)]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_cat(df, cat_cols,output_col=None):\n",
    "    \"\"\"\n",
    "    This function converts all categorical columns into numerical boolean columns.\n",
    "    \n",
    "    There are 3 parameters: df, cat_cols, and output_var. \n",
    "    \n",
    "    1. df is the dataframe which needs to have categorical variables converted to numerical variables\n",
    "    2. cat_cols needs to be a list that contains the names of all categorical columns that need to be converted.\n",
    "    3. output_var is the name of the output or response variable.  It is set to 'Output' as default.\"\"\"\n",
    "    \n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        dummy_col = pd.get_dummies(df_out[col],drop_first = True)\n",
    "        df_out.drop(col,axis=1, inplace = True)\n",
    "        df_out = df_out.join(dummy_col)\n",
    "    \n",
    "    if output_col != None:\n",
    "        loc_df_out = list(df_out.columns).index(output_col)\n",
    "        df_out = df_out[list(df_out.columns[:loc_df_out]) + list(df_out.columns[loc_df_out + 1:]) + list(df_out.columns[loc_df_out:loc_df_out + 1])]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(df,cols_to_ignore = None):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe as a parameter and returns the same dataframe with all the features normalized between 0 and 1 using rescaling (min-max normalization)\n",
    "    \"\"\"\n",
    "    l_min = []\n",
    "    l_max = []\n",
    "    desc = df.describe()\n",
    "    if cols_to_ignore == None:\n",
    "        for col in df.columns:\n",
    "            l_min.append(desc[col]['min'])\n",
    "            l_max.append(desc[col]['max'])\n",
    "        \n",
    "        t_min = list(zip(df.columns, l_min))\n",
    "        t_max = list(zip(df.columns, l_max))\n",
    " \n",
    "\n",
    "    else:\n",
    "        for col in df.drop(cols_to_ignore,axis = 1).columns:\n",
    "            l_min.append(desc[col]['min'])\n",
    "            l_max.append(desc[col]['max'])\n",
    "\n",
    "        t_min = list(zip(df.drop(cols_to_ignore,axis = 1).columns, l_min))\n",
    "        t_max = list(zip(df.drop(cols_to_ignore,axis = 1).columns, l_max))\n",
    "    \n",
    "   \n",
    "    d_min = {}\n",
    "    for col,val in t_min:\n",
    "        d_min[col]=val\n",
    "    \n",
    "    d_max = {}\n",
    "    for col,val in t_max:\n",
    "        d_max[col]=val\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    for key in d_min.keys():\n",
    "        df_copy[key] = df_copy[key].apply(lambda x: (x - d_min[key])/ (d_max[key] - d_min[key]))\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_df(df,test_size = 0.3,df_to_return = 'df_train'):\n",
    "    \"\"\"\n",
    "    This function takes in a Pandas DataFrame and returns a \n",
    "    dataframe that is a subset of that Pandas DataFrame.\n",
    "    \n",
    "    There are 3 parameters: df, test_size, and df_to_return\n",
    "    \n",
    "    df needs to be a Pandas DataFrame and is the superset dataframe to be divided.\n",
    "    test_size is the proportion of the dataframe you want to be the testing dataset.\n",
    "    test_size is set to 0.3 by default.\n",
    "    df_to_return needs to specified as either 'df_train' or df_test' \n",
    "    to return the correct subset dataframe. df_to_return is set to 'df_train' by default\n",
    "    \"\"\"\n",
    "    split_num = int(df.count()[0] * (1-test_size) //1)\n",
    "    df_train = df.iloc[:split_num,:]\n",
    "    df_test = df.iloc[split_num:,:]\n",
    "    if df_to_return in ['df_train','train']:\n",
    "        return df_train\n",
    "    elif df_to_return in ['df_test','test']:\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffled_split_dfs(df,test_size = 0.3):\n",
    "    \"\"\"\n",
    "    This function takes in a Pandas DataFrame and returns a list of 2\n",
    "    dataframes.  The first dataframe is the train and the second is the test df.\n",
    "    \n",
    "    There are 2 parameters: df and test_size\n",
    "    \n",
    "    df needs to be a Pandas DataFrame and is the superset dataframe to be divided.\n",
    "    test_size is the proportion of the dataframe you want to be the testing dataset.\n",
    "    test_size is set to 0.3 by default.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.sample(frac = 1).reset_index(drop = True)\n",
    "    split_num = int(df_copy.count()[0] * (1-test_size) //1)\n",
    "    df_train = df_copy.iloc[:split_num,:]\n",
    "    df_test = df_copy.iloc[split_num:,:]\n",
    "    return ([df_train,df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_accuracy(df = None,pred_df = None, test_df = None, algo = 'lin',target_class = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes in a pandas DataFrame and returns the accuracy of the model\n",
    "    \n",
    "    There are 5 parameters: df and algo\n",
    "    \n",
    "    1. df needs to be a Pandas DataFrame and algo is the algorithm used.\n",
    "    2. pred_df is the prediction dataframe used for the knn algorithm\n",
    "    3. test_df is the test dataframe used for the knn algorithm\n",
    "    4. algo is set to 'lin' by default but can also be specified as 'log' or 'knn'\n",
    "    5. target_class is the output_variable\n",
    "    \"\"\"\n",
    "    \n",
    "    if algo == 'lin':\n",
    "        df_out = df.copy()\n",
    "        df_out['error'] = df.iloc[:,-2] - df.iloc[:,-1]\n",
    "        ME = df_out['error'].mean()\n",
    "        MAE = np.abs(df_out['error']).mean()\n",
    "        RMSE = (sum(df_out['error']**2)/df_out.count()[0]+1) ** 0.5\n",
    "        return {'ME':ME,'MAE':MAE,'RMSE':RMSE}\n",
    "    \n",
    "    elif algo == 'log':\n",
    "        import numpy as np\n",
    "        from sklearn.metrics import confusion_matrix,classification_report\n",
    "        if target_class == None:\n",
    "            target_class = df.columns[-4]\n",
    "        print(confusion_matrix(df[target_class].values,df['Crisp'].values))\n",
    "        print(classification_report(df[target_class].values,df['Crisp'].values))\n",
    "    \n",
    "        return sum(df['Correct?']/df.count()[0])\n",
    "    \n",
    "    elif algo == 'knn':\n",
    "        pred = pred_df[target_class]\n",
    "        test = test_df[target_class]\n",
    "        return sum(pred == test) / len(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear Regression and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stoch_grad_desc(dataset,output_col,cols_to_ignore = None,alpha = 0.3,epoch = 10,algo = 'lin',initial_coeffs = 1):\n",
    "    \"\"\"\n",
    "    This function returns a list of the coefficients for the specified algorithm.  \n",
    "    Currently, this function only performs Linear and Logistic Regression.\n",
    "    \n",
    "    The 4 parameters are: dataset, alpha, epoch, and algo\n",
    "    \n",
    "    1. dataset needs to be a pandas DataFrame\n",
    "    2. alpha is the alpha value used in stochiastic gradient descent.  It is set at 0.3 by default.\n",
    "    3. epoch is the number of iterations through each row in the dataset algorithm will perform.  epoch is set to 10 by default.\n",
    "    4. algo is the specific algorithm to be used.  algo is 'lin' by default for Linear Regression but can also be specified as 'log' for Logistic Regression\n",
    "    \"\"\"\n",
    "\n",
    "    loc_output_col = list(dataset.columns).index(output_col)\n",
    "    dataset = dataset[list(dataset.columns[:loc_output_col]) + list(dataset.columns[loc_output_col+1:]) + list(dataset.columns[loc_output_col:loc_output_col + 1])]\n",
    "    \n",
    "    from math import exp\n",
    "    count_rows = dataset.count()[1]\n",
    "    \n",
    "    if cols_to_ignore != None:\n",
    "        dataset_1 = pd.DataFrame(pd.Series(np.ones(dataset.count()[0])),columns = ['X0']).join(dataset.drop(cols_to_ignore,axis = 1))\n",
    "    else:\n",
    "        dataset_1 = pd.DataFrame(pd.Series(np.ones(dataset.count()[0])),columns = ['X0']).join(dataset)\n",
    "        \n",
    "    coeffs = list(np.ones(len(dataset_1.columns[0:-1])))\n",
    "    coeffs = [i * initial_coeffs for i in coeffs]\n",
    "    \n",
    "    df_columns = list(dataset_1.columns)\n",
    "    df_input_cols = df_columns\n",
    "    df_input_cols.remove(output_col)\n",
    "    \n",
    "    for ep in range(epoch):\n",
    "        for row in range(count_rows):\n",
    "            y = dataset_1.loc[row,output_col]\n",
    "            output_terms = []\n",
    "            #return y\n",
    "            for col in df_input_cols:\n",
    "                output_terms.append((coeffs[dataset_1.columns.get_loc(col)],dataset_1.loc[row,col]))\n",
    "            \n",
    "            output_list = [(x*y) for (x,y) in output_terms]\n",
    "            \n",
    "            output = sum(output_list)\n",
    "            \n",
    "            if algo == 'lin':\n",
    "                pred = output\n",
    "                for i in range(len(coeffs)):\n",
    "                    coeffs[int(i)] += alpha * (dataset_1.loc[row,output_col] - pred) * dataset_1.iloc[row,int(i)]\n",
    "                \n",
    "            elif algo == 'log':\n",
    "                pred = 1 / (1 + exp(-output))\n",
    "                \n",
    "                for i in range(len(coeffs)):\n",
    "                    coeffs[i] = coeffs[i] + alpha * (y - pred) * pred * (1 - pred) * dataset_1.iloc[row,i]\n",
    "                    \n",
    "    \n",
    "    \n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(dataset, coeff_list, output_col,cols_to_ignore = None,algo = 'lin'):\n",
    "    \"\"\"\n",
    "    This function takes in a pandas DataFrame and a list that contains \n",
    "    coefficients for the specified algorith used in the stoch_grad_desc function\n",
    "    and returns the same dataset (with the addition of the first column being 1s to\n",
    "    represent X0 in the regression formula) plus a new column at the end, 'Prediction'.\n",
    "    \n",
    "    There are 3 parameters: dataset, coeff_list, and algo\n",
    "    \n",
    "    1. dataset is the dataframe to used to make predictions dataset needs to be a Pandas DataFrame\n",
    "    2. coeff_list should be the list that was the result of running the stoch_grad_desc function\n",
    "    3. algo is the specific algorithm to be used.  algo is set to 'lin' by default but can be set to 'log'\"\"\"\n",
    "    \n",
    "    loc_output_col = list(dataset.columns).index(output_col)\n",
    "    dataset = dataset[list(dataset.columns[:loc_output_col]) + list(dataset.columns[loc_output_col+1:]) + list(dataset.columns[loc_output_col:loc_output_col + 1])]\n",
    "    dataset_index = dataset.index\n",
    "    \n",
    "    if type(cols_to_ignore) == list and cols_to_ignore != None:\n",
    "        df_ignored_cols = dataset.loc[:,cols_to_ignore]\n",
    "    elif type(cols_to_ignore) != list and cols_to_ignore != None:\n",
    "        df_ignored_cols = dataset.loc[:,[cols_to_ignore]]\n",
    "    if cols_to_ignore != None:\n",
    "        df_ignored_cols.reset_index(inplace = True)\n",
    "    \n",
    "    from math import exp\n",
    "    \n",
    "    dataset.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    if cols_to_ignore != None:\n",
    "        dataset_out = pd.DataFrame(pd.Series(np.ones(dataset.count()[0]))).join(dataset.drop(cols_to_ignore,axis = 1))\n",
    "    else:\n",
    "        dataset_out = pd.DataFrame(pd.Series(np.ones(dataset.count()[0]))).join(dataset)\n",
    "\n",
    "    \n",
    "    dataset_out.rename(mapper = {0:'X0'},axis = 1, inplace = True)\n",
    "    \n",
    "    coeffs = coeff_list\n",
    "    pred = []\n",
    "    \n",
    "    for row in range(dataset_out.count()[0]):\n",
    "        output_terms = []\n",
    "        for col in dataset_out.columns[0:-1]:\n",
    "            output_terms.append((coeffs[dataset_out.columns.get_loc(col)],dataset_out.loc[row,col]))\n",
    "        output_list = [x*y for (x,y) in output_terms]\n",
    "        \n",
    "        output = sum(output_list)\n",
    "        if algo == 'lin':\n",
    "            pred.append(output)\n",
    "        \n",
    "        elif algo == 'log':\n",
    "            pred.append(1/(1 + exp(-output)))\n",
    "    dataset_out = dataset_out.join(pd.DataFrame(pred))\n",
    "    dataset_out.rename(mapper = {0: 'Prediction'},axis = 1, inplace = True)\n",
    "    \n",
    "    if algo == 'log':\n",
    "        dataset_out['Crisp'] = dataset_out['Prediction'].apply(lambda predi: 1 if predi >= 0.5 else 0)\n",
    "        dataset_out['Correct?'] = dataset_out.iloc[:,-3] == dataset_out['Crisp']\n",
    "    \n",
    "    if cols_to_ignore != None:\n",
    "        dataset_out = pd.concat([df_ignored_cols,dataset_out],axis = 1)\n",
    "    dataset_out.drop('X0',axis = 1,inplace = True)\n",
    "    \n",
    "    dataset_out.set_index(dataset_index,inplace = True)\n",
    "    return dataset_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_1pt(df,new_point,output_col,k = 3):\n",
    "    import statistics\n",
    "    df1 = df.copy()\n",
    "    new_pt_df = pd.DataFrame(data = [new_point + ['DK']],columns=df.columns)\n",
    "    df_out = df1.append(new_pt_df,ignore_index = True)\n",
    "    \n",
    "    df2 = df_out.drop(output_col,axis=1)\n",
    "    count_rows = df2.count()[0]\n",
    "    new_pt_ind = count_rows - 1\n",
    "    df2['sum_sqrd_diffs'] = 0\n",
    "    for row_num in range(0,count_rows):\n",
    "        sum_sqrd_diffs = 0\n",
    "        for col_num in range(0,len(df2.columns)):\n",
    "            sum_sqrd_diffs += (df2.iloc[new_pt_ind,col_num] - df2.iloc[row_num,col_num])**2\n",
    "        df_out.loc[row_num,'sum_sqrd_diffs'] = sum_sqrd_diffs \n",
    "       \n",
    "    df_out.loc[new_pt_ind,output_col] = statistics.mode(df_out.iloc[:new_pt_ind,:].sort_values('sum_sqrd_diffs').head(k)[output_col])\n",
    "    #return df_out.sort_values('sum_sqrd_diffs').head(10)\n",
    "    return df_out\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn(df_train,new_pts_list,dep_col,k = 7,only_pred_df = True):\n",
    "    from time import time\n",
    "    start_time = time()\n",
    "    import statistics\n",
    "    \n",
    "    nrows_new_pts = new_pts_list.shape[0]\n",
    "    loc_dep_col = list(df_train.columns).index(dep_col)\n",
    "    df_in = df_train[list(df_train.columns)[0:loc_dep_col] + list(df_train.columns)[loc_dep_col + 1:] + list(df_train.columns)[loc_dep_col:loc_dep_col + 1]] \n",
    "    \n",
    "    if type(new_pts_list) == type(df_in):\n",
    "        l_list_new_pts = []\n",
    "        \n",
    "        for rn in range(new_pts_list.count()[0]):\n",
    "            l_list_new_pts.append(list(new_pts_list.iloc[rn,:]))\n",
    "        new_pts_list = l_list_new_pts\n",
    "\n",
    "    def knn_1pt(df,new_point,output_col = dep_col,k = k):\n",
    "        \n",
    "        df1 = df.copy()\n",
    "        new_pt_df = pd.DataFrame(data = [new_point + ['DK']],columns=df1.columns)\n",
    "        df_out = df1.append(new_pt_df,ignore_index = True)\n",
    "\n",
    "        df2 = df_out.drop(output_col,axis=1)\n",
    "        count_rows = df2.count()[0]\n",
    "        new_pt_ind = count_rows - 1\n",
    "        df2['sum_sqrd_diffs'] = 0\n",
    "        for row_num in range(0,count_rows):\n",
    "            sum_sqrd_diffs = 0\n",
    "            for col_num in range(0,len(df2.columns)):\n",
    "                sum_sqrd_diffs += (df2.iloc[new_pt_ind,col_num] - df2.iloc[row_num,col_num])**2\n",
    "            df_out.loc[row_num,'sum_sqrd_diffs'] = sum_sqrd_diffs \n",
    "            \n",
    "        \n",
    "        if k == 1:\n",
    "            df_out.loc[new_pt_ind,output_col] = statistics.mode(df_out.iloc[:new_pt_ind,:].sort_values('sum_sqrd_diffs').head(k)[output_col])\n",
    "            df_in = df_out.drop('sum_sqrd_diffs',axis = 1).copy()\n",
    "        else:\n",
    "            try:\n",
    "                mode = statistics.mode(df_out.sort_values('sum_sqrd_diffs')[0:k][output_col])\n",
    "            except:\n",
    "                closest = list(df_out.sort_values('sum_sqrd_diffs')[0:k][output_col].values)\n",
    "                copy = closest[:]\n",
    "                closest.sort(key = lambda x:copy.count(x))\n",
    "                closest.reverse()\n",
    "                mode = closest[0]\n",
    "            df_out.loc[new_pt_ind,output_col] = mode\n",
    "            df_in = df_out.drop('sum_sqrd_diffs',axis = 1).copy()\n",
    "        return df_in\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    for np in new_pts_list:\n",
    "        df_in =  knn_1pt(new_point = np,df=df_in).copy()\n",
    "    \n",
    "    end_time = time()\n",
    "    print(\"Program took %s seconds to run.\" % (end_time - start_time))\n",
    "    if only_pred_df == True:\n",
    "        return df_in[-nrows_new_pts:]\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans(df,k=3,epoch = 1):\n",
    "    import random\n",
    "    \n",
    "    def euclid_squared_distance(pt1,pt2):\n",
    "        return (pt1 - pt2)**2\n",
    "    \n",
    "    def select_k_points(dataframe = df,k = k):\n",
    "        centroid_dataframe = pd.DataFrame(columns = dataframe.columns)\n",
    "        centroid_indices = []\n",
    "        for i in range(k):\n",
    "            while True:\n",
    "                rand_ind = random.choice(list(dataframe.index))\n",
    "                if rand_ind not in centroid_indices:\n",
    "                    centroid_indices.append(rand_ind)\n",
    "                    break\n",
    "            centroid_dataframe = centroid_dataframe.append(pd.DataFrame(data = [list(dataframe.loc[rand_ind,:])],columns = dataframe.columns,index = [rand_ind]))\n",
    "            for row_num,ind in enumerate(list(centroid_dataframe.index)):\n",
    "                centroid_dataframe.loc[ind,'Cluster'] = int(row_num)\n",
    "        return centroid_dataframe\n",
    "    \n",
    "#     def kmeans_pp(dataframe = df,k = k):\n",
    "#         centroid_dataframe = pd.DataFrame(columns = dataframe.columns)\n",
    "#         centroid_indices = []\n",
    "#         rand_ind = random.choice(list(dataframe.index))\n",
    "#         centroid_indices.append(rand_ind)\n",
    "#         centroid_dataframe.loc[rand_ind,:] = dataframe.iloc[rand_ind,:]\n",
    "        \n",
    "#         for i in range(k-1):\n",
    "#             for row in dataframe.index:\n",
    "#                 sqd_diffs_df = pd.DataFrame(columns = dataframe.columns)\n",
    "#                 for cent_ind in centroid_indices:                \n",
    "#                     sqd_diffs_df.loc[cent_ind,:] = (centroid_dataframe.loc[cent_ind,:] - dataframe.loc[row,:]) ** 2\n",
    "#                     sqd_diffs_df['Sum_Squared_Diffs'] = sqd_diffs_df.sum(axis = 1)\n",
    "#                 min_sqd_diffs = sqd_diffs_df['Sum_Squared_Diffs'].min()\n",
    "#                 sqd_diffs_df=sqd_diffs_df[sqd_diffs_df['Sum_Squared_Diffs'] = 8]\n",
    "\n",
    "            \n",
    "    df['Cluster'] = np.nan\n",
    "    \n",
    "    centroid_df = select_k_points()\n",
    "\n",
    "    centroid_indices = list(centroid_df.index)\n",
    "    \n",
    "    \n",
    "\n",
    "    def one_iteration_k_means(dataframe = df,centroid_dataframe = centroid_df,cluster_col = 'Cluster'):\n",
    "        for row in dataframe.index:\n",
    "            sqd_diffs_df = pd.DataFrame(columns = centroid_dataframe.drop(cluster_col,axis = 1).columns)\n",
    "            for cent_ind in centroid_indices:\n",
    "                sqd_diffs_df.loc[cent_ind,:] = (centroid_dataframe.drop(cluster_col,axis = 1).loc[cent_ind,:] - dataframe.drop(cluster_col,axis = 1).loc[row,:]) ** 2 \n",
    "            sqd_diffs_df['Sum_Sqd_Diffs'] = sqd_diffs_df.sum(axis = 1)\n",
    "            for row_num,ind in enumerate(list(sqd_diffs_df.index)):\n",
    "                sqd_diffs_df.loc[ind,'Cluster'] = int(row_num)\n",
    "\n",
    "            smallest_ssd = sqd_diffs_df['Sum_Sqd_Diffs'].min()\n",
    "\n",
    "            smallest_ssd_df = sqd_diffs_df[sqd_diffs_df['Sum_Sqd_Diffs'] == smallest_ssd]\n",
    "\n",
    "            closest_centroid = pd.DataFrame(data = [smallest_ssd_df.loc[smallest_ssd_df.index[0],:].values],columns = list(sqd_diffs_df.columns),index = smallest_ssd_df.index)\n",
    "\n",
    "            closest_centroid_ind = list(closest_centroid.index)[0]\n",
    "\n",
    "            df.loc[row,cluster_col] = centroid_dataframe.loc[closest_centroid_ind,cluster_col]\n",
    "\n",
    "            for row_cent in list(centroid_dataframe.index):\n",
    "                \n",
    "                if row_cent == closest_centroid_ind:\n",
    "                    for col in list(centroid_dataframe.drop(cluster_col,axis = 1).columns):\n",
    "                        centroid_dataframe.loc[row_cent,col] = (centroid_dataframe.loc[row_cent,col] + df.loc[row,col])/2\n",
    "                        \n",
    "        return centroid_dataframe\n",
    "    \n",
    "    for iteration in range(epoch):\n",
    "        centroid_df = one_iteration_k_means() \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cart(df):\n",
    "    feat_1 = df.columns[0]\n",
    "    feat_2 = df.columns[1]\n",
    "    target = df.columns[-1]\n",
    "    def gini(feat,row,dataset = df):\n",
    "        df_gini = df.copy()\n",
    "        #return df_gini.loc[1,feat]\n",
    "\n",
    "        for i in range(df_gini.count()[0]):\n",
    "            if df_gini.loc[i,feat] < df_gini.loc[row,feat]:\n",
    "                df_gini.loc[i,'Group'] = 0\n",
    "            else:\n",
    "                df_gini.loc[i,'Group'] = 1\n",
    "        try:\n",
    "            count_left = df_gini['Group'].value_counts()[0]\n",
    "        except:\n",
    "            count_left = 0\n",
    "        try:\n",
    "            count_right = df_gini['Group'].value_counts()[1]\n",
    "        except:\n",
    "            count_right = 0\n",
    "        \n",
    "        count_0_left = df_gini[(df_gini[target] == df_gini['Group']) & (df_gini[target] == 0)].count()[0]\n",
    "        count_1_left = df_gini[(df_gini[target] != df_gini['Group']) & (df_gini[target] == 1)].count()[0]\n",
    "        count_0_right = df_gini[(df_gini[target] != df_gini['Group']) & (df_gini[target] == 0)].count()[0]\n",
    "        count_1_right = df_gini[(df_gini[target] == df_gini['Group']) & (df_gini[target] == 1)].count()[0]\n",
    "\n",
    "        g_split = ((count_0_left/count_left) * (1 - count_0_left/count_left))+ \\\n",
    "                   ((count_0_right/count_right) * (1 - count_0_right/count_right)) + \\\n",
    "                   ((count_1_left/count_left) * (1 - count_1_left/count_left)) + \\\n",
    "                   ((count_1_right/count_right) * (1 - count_1_right/count_right)) \n",
    "        return g_split\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i in range(0,df.count()[0]):\n",
    "        gini_split = gini(dataset = df,feat = feat_1,row = i)\n",
    "        l.append((i,feat_1,df.loc[i,feat_1],gini_split))\n",
    "        \n",
    "    for i in range(0,df.count()[0]):\n",
    "        gini_split = gini(dataset = df,feat = feat_2,row = i)\n",
    "        l.append((i,feat_2,df.loc[i,feat_2],gini_split))\n",
    "    l_gini = [i[-1] for i in l]\n",
    "    gini_min = min(l_gini)\n",
    "    l_final = list(filter(lambda x: gini_min == x[-1],l))\n",
    "    \n",
    "    return l_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
