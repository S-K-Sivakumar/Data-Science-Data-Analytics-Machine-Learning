{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of functions for Machine Learning:\n",
      "\n",
      "1. impute_cols\n",
      "\n",
      "2. Optional: remove_high_corr_cols and/or combine_dummy_variable_columns\n",
      "\n",
      "3. convert_cat\n",
      "\n",
      "4. find_cols_to_ignore (to be used only with normalize_df function)\n",
      "\n",
      "5. shuffled_split_dfs / split_dfs (returns a list of two dataframes, train_df and test_df)\n",
      "\n",
      "6. normalize (need to do for both train_df and test_df separately so there is no data leakage from test_df into train_df)\n",
      "\n",
      "7. stoch_grad_desc (input train_df as the dataset parameter)\n",
      "\n",
      "8. make_predictions (input test_df as the dataset parameter)\n",
      "\n",
      "9. check_accuracy\n"
     ]
    }
   ],
   "source": [
    "print(\"Order of functions for Machine Learning:\\n\\n1. impute_cols\\n\\n2. Optional: remove_high_corr_cols and/or combine_dummy_variable_columns\\n\\n3. convert_cat\\n\\n4. find_cols_to_ignore (to be used only with normalize_df function)\\n\\n5. shuffled_split_dfs / split_dfs (returns a list of two dataframes, train_df and test_df)\\n\\n6. normalize (need to do for both train_df and test_df separately so there is no data leakage from test_df into train_df)\\n\\n7. stoch_grad_desc (input train_df as the dataset parameter)\\n\\n8. make_predictions (input test_df as the dataset parameter)\\n\\n9. check_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Fuctions Across Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_random_df(model,nrows,ncols,nclasses = None,target_multiplier = 1):\n",
    "    \"\"\"\n",
    "    Creates a random Pandas DataFrame based on input values for model,nrows,ncols,nclasses,andtarget_multiplier\n",
    "    \"\"\"\n",
    "    \n",
    "    features = ['feat_%s' % col for col in range(1,ncols)]\n",
    "    #return [features + ['Target']]\n",
    "    df = pd.DataFrame(np.random.rand(nrows,ncols),columns = features + ['Target'])\n",
    "    #return df\n",
    "    if model.lower() in ['lin','linear regression','reg','regression']:\n",
    "        df['Target'] *= target_multiplier\n",
    "    else:\n",
    "        if model.lower() in ['log','logistic regression','logreg']:\n",
    "            nclasses = 2\n",
    "        df['Target'] = df['Target'].apply(lambda x:np.random.choice(range(nclasses)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_cols_to_ignore(df,thresh_cat = 0.02,cols_to_ignore = None):\n",
    "    \"\"\"\n",
    "    Finds columns to ignore for normalization\n",
    "    ONLY USE FOR normalize FUNCTION, NOT FOR impute_cols FUNCTION\n",
    "    \"\"\"\n",
    "    \n",
    "    if cols_to_ignore == None:\n",
    "        cols_to_ignore = []\n",
    "    elif type(cols_to_ignore) == str:\n",
    "        cols_to_ignore = [cols_to_ignore]\n",
    "    cols = list(df.columns)\n",
    "    for col in cols:\n",
    "        if df[col].dtype.name not in ['int32','int64','float32','float64'] and col not in cols_to_ignore:\n",
    "            cols_to_ignore.append(col)\n",
    "        elif df[col].dtype.name in ['int32','int64','float32','float64'] and col not in cols_to_ignore:\n",
    "            num_unique = df[col].value_counts().count()\n",
    "            num_total = df[col].count()\n",
    "            if (num_unique / num_total) < thresh_cat:\n",
    "                cols_to_ignore.append(col)\n",
    "    return list(set(cols_to_ignore))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def impute_cols(df,k=None,null_cols = 'auto_select',thresh_null_cat = 0.02,cols_to_ignore = None):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe, imputes all the columns which have nulls, and returns a fully cleansed dataframe\n",
    "    Parameters:\n",
    "    1. df: a Pandas DataFrame\n",
    "    2. k: this parameter is the k number of neighbors used in the KNN algorithm to classify null values in a categorical column.\n",
    "       If k == None, then k will be calculated for each null_cat_column as the square root of the number of \n",
    "       non null values (labeled values) in that particular column. Default is None.\n",
    "    3. null_cols: default is 'auto_select' meaning function automatically will impute all columns with null values.  \n",
    "       Can specify one column as a string or one or more multiple columns as a list instead.\n",
    "    4. thresh_null_cat: this parameter determines the minimum ratio of unique_non_null_non_zero values to total_non_null_non_zero\n",
    "       values for each column in order for that column to count as a discrete column. Default is 0.03\n",
    "    5. cols_to_ignore: columns that should be ignored when normalizing.  Non-numerical columns will automatically be ignored.\n",
    "       Default is None.\n",
    "       \n",
    "       **NOTE: IF ALL THE COLUMNS IN THE DATAFRAME ARE CATEGORICAL, NEED TO INCLUDE NUMBERICAL COLUMNS FOR THIS FUNCTION TO WORK.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### This function determines the number of unique values in each column passed in the parameter and return a dictionary\n",
    "    ### containing the column names as keys and the unique values as values\n",
    "    from time import time\n",
    "    func_start_time = time()\n",
    "    def num_unique_values_in_cols(df,cols = None): \n",
    "        if type(cols) == str:\n",
    "            cols = [cols]\n",
    "        elif cols == None:\n",
    "            cols = list(df.columns)\n",
    "\n",
    "        d_unique = {}\n",
    "        for col in cols:\n",
    "            d_unique[col] = df[col].value_counts().count()\n",
    "        return d_unique\n",
    "    \n",
    "    ### This function is the KNN algorithm used to impute categorical columns that contain null values\n",
    "    def knn(df_train,new_pts_list,dep_col,k = k,only_pred_df = True):\n",
    "        start_time = time()\n",
    "        import statistics\n",
    "        import numpy as np\n",
    "\n",
    "        nrows_new_pts = new_pts_list.shape[0]\n",
    "        loc_dep_col = list(df_train.columns).index(dep_col)\n",
    "        df_in = df_train[list(df_train.columns)[0:loc_dep_col] + list(df_train.columns)[loc_dep_col + 1:] + list(df_train.columns)[loc_dep_col:loc_dep_col + 1]] \n",
    "\n",
    "        if type(new_pts_list) == type(df_in):\n",
    "            l_list_new_pts = []\n",
    "\n",
    "            for rn in range(new_pts_list.count()[0]):\n",
    "                l_list_new_pts.append(list(new_pts_list.iloc[rn,:]))\n",
    "            new_pts_list = l_list_new_pts\n",
    "\n",
    "        def knn_1pt(df,new_point,output_col = dep_col,k = k):\n",
    "\n",
    "            df1 = df.copy()\n",
    "            new_pt_df = pd.DataFrame(data = [new_point + ['DK']],columns=df1.columns)\n",
    "            #return new_pt_df\n",
    "            df_out = df1.append(new_pt_df,ignore_index = True)\n",
    "            #return df_out\n",
    "\n",
    "            df2 = df_out.drop(output_col,axis=1)\n",
    "            #return df2\n",
    "            count_rows = df2.count()[0]\n",
    "            new_pt_ind = count_rows - 1\n",
    "\n",
    "            dist_list = np.square((np.matrix(df2[:new_pt_ind]) - np.array(df2[new_pt_ind:]))).sum(axis = 1).transpose()[0].tolist()[0]\n",
    "            #return dist_list\n",
    "            enum_list = list(enumerate(dist_list))\n",
    "            enum_list.sort(key=lambda x:x[1])\n",
    "            #return enum_list\n",
    "            top_tup_list = enum_list[:k]\n",
    "            #return top_tup_list\n",
    "            closest_ind_list = [ind for (ind,dist) in enum_list[0:k]]\n",
    "            labels_list = list(df_out.loc[closest_ind_list,output_col].values)\n",
    "            #return labels_list\n",
    "            sorted_labels = sorted(labels_list,key = labels_list.count,reverse = True)\n",
    "            #return sorted_labels\n",
    "            if k == 1:\n",
    "                df_out.loc[new_pt_ind,output_col] = sorted_labels[0]\n",
    "            else:\n",
    "                try:\n",
    "                    mode = statistics.mode(sorted_labels)\n",
    "                except:\n",
    "                    mode = sorted_labels[0]\n",
    "\n",
    "                finally:\n",
    "                    df_out.loc[new_pt_ind,output_col] = mode\n",
    "            return df_out\n",
    "\n",
    "\n",
    "        for new_point in new_pts_list:\n",
    "            df_in =  knn_1pt(new_point = new_point,df=df_in).copy()\n",
    "\n",
    "        end_time = time()\n",
    "        run_time = end_time - start_time\n",
    "\n",
    "        mins = run_time // 60\n",
    "        secs = run_time % 60\n",
    "        hours = mins // 60\n",
    "        mins = mins % 60\n",
    "\n",
    "        print(\"%s column took %s hr(s), %s min(s), %s sec(s) to be imputed\" % (dep_col,hours,mins,secs))    \n",
    "\n",
    "        if only_pred_df == True:\n",
    "            return df_in[-nrows_new_pts:]\n",
    "        return df_in    \n",
    "    \n",
    "    \n",
    "    ### This is to separate the null_value_columns into two separate lists: 1. Categorical column names 2. Discrete column names    \n",
    "    if null_cols == 'auto_select':\n",
    "        null_cols = list(df.isna().sum()[df.isna().sum() != 0].sort_values(ascending = False).index)\n",
    "    elif type(null_cols) != list:\n",
    "        null_cols = [null_cols]\n",
    "    null_cat_cols = []\n",
    "    null_discrete_cols = []\n",
    "    \n",
    "    d_unique_in_col = num_unique_values_in_cols(df = df,cols = null_cols)\n",
    "    for col in null_cols:\n",
    "        if df[col].dtype.name not in ('int32','int64','float32','float64'):\n",
    "            null_cat_cols.append(col)\n",
    "            continue\n",
    "        else:\n",
    "            num_nulls = {}\n",
    "            count_non_nulls_non_zeros = df[col].count() - df[df[col] == 0].count()[col]\n",
    "            #num_nulls[col] = df.isna().sum()[col]\n",
    "            if (d_unique_in_col[col] / count_non_nulls_non_zeros) <= thresh_null_cat:\n",
    "                null_cat_cols.append(col)\n",
    "            else:\n",
    "                null_discrete_cols.append(col)\n",
    "            \n",
    "    def normalize(df = df,null_cat_cols = null_cat_cols,cols_to_ignore = cols_to_ignore):\n",
    "        \"\"\"\n",
    "        This function takes in a dataframe as a parameter and returns the same dataframe with all the features normalized between 0 and 1 using rescaling (min-max normalization)\n",
    "        \"\"\"\n",
    "        if cols_to_ignore == None:\n",
    "            cols_to_ignore = []\n",
    "            \n",
    "        elif type(cols_to_ignore) == str:\n",
    "            cols_to_ignore = [cols_to_ignore]\n",
    "\n",
    "        cols_to_ignore = cols_to_ignore + null_cat_cols\n",
    "        cols_to_ignore = list(set(cols_to_ignore))\n",
    "\n",
    "        for col in list(df.columns):\n",
    "            if col not in cols_to_ignore and df[col].dtype.name not in ('int32','int64','float32','float64'):\n",
    "                cols_to_ignore.append(col)\n",
    "\n",
    "        l_min = []\n",
    "        l_max = []\n",
    "        desc = df.describe()\n",
    "        if cols_to_ignore == None:\n",
    "            for col in df.columns:\n",
    "                l_min.append(desc[col]['min'])\n",
    "                l_max.append(desc[col]['max'])\n",
    "\n",
    "            t_min = list(zip(df.columns, l_min))\n",
    "            t_max = list(zip(df.columns, l_max))\n",
    "\n",
    "\n",
    "        else:\n",
    "            for col in df.drop(cols_to_ignore,axis = 1).columns:\n",
    "                l_min.append(desc[col]['min'])\n",
    "                l_max.append(desc[col]['max'])\n",
    "\n",
    "            t_min = list(zip(df.drop(cols_to_ignore,axis = 1).columns, l_min))\n",
    "            t_max = list(zip(df.drop(cols_to_ignore,axis = 1).columns, l_max))\n",
    "\n",
    "\n",
    "        d_min = {}\n",
    "        for col,val in t_min:\n",
    "            d_min[col]=val\n",
    "\n",
    "        d_max = {}\n",
    "        for col,val in t_max:\n",
    "            d_max[col]=val\n",
    "\n",
    "        df_copy = df.copy()\n",
    "        for key in d_min.keys():\n",
    "            df_copy[key] = df_copy[key].apply(lambda x: (x - d_min[key])/ (d_max[key] - d_min[key]))\n",
    "\n",
    "        return cols_to_ignore,df_copy\n",
    "\n",
    "    cols_to_ignore,norm_df = normalize()\n",
    "    ### Now that the two lists containing the discrete null columns and the categorical null columns are created, I am going to\n",
    "    ### impute the categorical columns first using the KNN algorithm\n",
    "    \n",
    "    #return null_cat_cols,null_discrete_cols\n",
    "    def impute_cat_cols(df = df,null_cat_cols = null_cat_cols,k = k,cols_to_ignore= cols_to_ignore):\n",
    "        new_df = df.copy()\n",
    "        for col in null_cat_cols:  \n",
    "            df_w_null = new_df.select_dtypes(include=numerics)\n",
    "            for ig_col in cols_to_ignore:\n",
    "                if col != ig_col:\n",
    "                    try:\n",
    "                        df_w_null.drop(col,axis = 1,inplace = True)\n",
    "                    except:\n",
    "                        pass\n",
    "            df_w_null[col] = new_df[col]\n",
    "            df_labeled = df_w_null.dropna()\n",
    "            if k == None:\n",
    "                k = int(np.round((df_labeled.count()[col]) ** .5))\n",
    "            df_unlabeled = df_w_null[df_w_null.isna()[col]].drop(col,axis = 1)\n",
    "            unlabeled_index = df_unlabeled.index\n",
    "            df_result = knn(df_labeled,df_unlabeled,dep_col = col,k = k)\n",
    "            df_result.set_index(unlabeled_index,inplace = True)\n",
    "            for ind,pred in list(zip(list(unlabeled_index),list(df_result[col]))):\n",
    "                new_df.loc[ind,col] = pred\n",
    "        return new_df\n",
    "    \n",
    "    ### df_out contains a dataframe where all the categorical columns have no null values but the discrete columns might still have nulls\n",
    "    df_out = impute_cat_cols()\n",
    "    \n",
    "    ### Now that the categorical columns are cleaned, I am going to impute the discrete columns\n",
    "    def impute_discrete_cols(df_disc = df_out,null_discrete_cols = null_discrete_cols,k=k):\n",
    "        for col in null_discrete_cols:\n",
    "            col_mean = np.round(df_disc[col].mean())\n",
    "            df_disc[col] = df_disc[col].fillna(value=col_mean)\n",
    "        return df_disc\n",
    "    \n",
    "    ### df_clean will have zero null values and will be returned from this overall function\n",
    "    df_clean = impute_discrete_cols()\n",
    "    \n",
    "    func_end_time = time()\n",
    "    run_time = func_end_time - func_start_time\n",
    "\n",
    "    mins = run_time // 60\n",
    "    secs = run_time % 60\n",
    "    hours = mins // 60\n",
    "    mins = mins % 60\n",
    "\n",
    "    print(\"Imputing took %s hr(s), %s min(s), %s sec(s) long.\" % (hours,mins,secs))    \n",
    "\n",
    "    return df_clean\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_unique_values_in_cols(df,cols = None):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe,df, and a list of cols (can be a string of one object),cols, and return a dictionary\n",
    "    with each column name in col as a key and the number of unique values in each column as a value\n",
    "    \"\"\"\n",
    "    if type(cols) == str:\n",
    "        cols = [cols]\n",
    "    elif cols == None:\n",
    "        cols = list(df.columns)\n",
    "    \n",
    "    d_unique = {}\n",
    "    for col in cols:\n",
    "        d_unique[col] = df[col].value_counts().count()\n",
    "    return d_unique\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df_in,outlier_cols = None,thresh_cat = 0.02,remove = True):\n",
    "    \"\"\"\n",
    "    This function removes outliers from the dataframe input, df_in, for the given columns, outlier_cols.  \n",
    "    It returns a dataframe with no outliers.  If remove is True, outliers are removed row-wise.  Else, outliers are replace by \n",
    "    q3 + 1.5 * IQR if above maximum allowed range or q1 - 1.5 * IQR if below.  thresh_cat is used to determine if a numerical\n",
    "    column is categorical.  All categorical columns will be ignored in outlier treatment.\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "    if type(outlier_cols) == str:\n",
    "        outlier_cols = [outlier_cols]\n",
    "    elif outlier_cols == None:\n",
    "        outlier_cols = list(df.columns)\n",
    "    \n",
    "    desc = df.describe()\n",
    "    outlier_cols_iter = []\n",
    "    for col in outlier_cols:\n",
    "        if df[col].dtype.name in ['int32','int64','float32','float64']:\n",
    "            num_unique = df[col].value_counts().count()\n",
    "            num_total = df[col].count()\n",
    "            if (num_unique / num_total) > thresh_cat:\n",
    "                outlier_cols_iter.append(col)\n",
    "              \n",
    "    for col in outlier_cols_iter:\n",
    "        q1 = desc.loc['25%',col]\n",
    "        q3 = desc.loc['75%',col]\n",
    "        iqr = q3 - q1\n",
    "        low = q1 - (1.5 * iqr)\n",
    "        high = q3 + (1.5 * iqr)\n",
    "        if remove == True:\n",
    "            df = df[(df[col] >= low) & (df[col] <= high)]\n",
    "        else:\n",
    "            def replace_outliers(val,low=low,high=high):\n",
    "                if val < low:\n",
    "                    return low\n",
    "                elif val > high:\n",
    "                    return high\n",
    "                else:\n",
    "                    return val\n",
    "            df[col] = df[col].apply(replace_outliers)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_high_corr_cols(dataframe,cols_to_use = None,cols_to_avoid = None,num_corr_cols_tolerated = None,max_corr_tolerated = 0.8,num_iterations = 5,type_corr = 'r_squared'):\n",
    "    \"\"\"remove_high_corr_cols removes columns in dataframe that are highly correlated with other columns.\n",
    "    This is to reduce multicollinearity between features which can cause overfitting in a regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    1. dataframe: Pandas DataFrame\n",
    "    2. cols_to_use: a list of columns which should be used for consideration in determining if multicollinearity is present.\n",
    "    IMPORTANT: Should be a list of all features and should exclude dependent variable. Default = None\n",
    "    3. cols_to_avoid: a list of columns which should be included in consideration in determining if multicollinearity is present.\n",
    "    IMPORTANT: Should be a list of multiple columns (must include dependent variable) or a string containing the dependent variable.\n",
    "    Default = None\n",
    "    4. num_corr_cols_tolerated: Maximum tolerated number of correlated columns tolerated in first iteration of inner function, \n",
    "    remove_high_corr_cols_one_iter. Default =None.\n",
    "    5. max_corr_tolerated: Maximum R squared value tolerated between features in cols_to_use list. Default = 0.8. \n",
    "    Default set to 5 if type_corr = 'vif'. vif = (1 / (1-R^2)). If R^2 = 0.8, vif = 5.\n",
    "    6. num_iterations: Number of iterations of inner function remove_high_corr_cols_one_iter. Default = 5\n",
    "    7. type_corr: Type of correlation coefficient used: Pearson coefficient squared (R^2) or variance inflation factor which is a measure of multicollinearity between features. \n",
    "    vif<5 (meaning R2 < 0.8) is a general threshold indicating less multicollinearity. Default = 'r_squared'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if num_corr_cols_tolerated <= num_iterations:\n",
    "        num_corr_cols_tolerated = num_iterations + 1\n",
    "        \n",
    "    def remove_high_corr_cols_one_iter(dataframe = dataframe,cols_to_use = cols_to_use,cols_to_avoid = cols_to_avoid,num_corr_cols_tolerated = num_corr_cols_tolerated,max_corr_tolerated = max_corr_tolerated,type_corr = type_corr):\n",
    "        ### Setting the type_corr variable correctly so that the correct correlation variable is calculated\n",
    "        if type_corr.lower() in ['vif','variance inflation factor','variance_inflation_factor']:\n",
    "            type_corr == 'vif'\n",
    "            if max_corr_tolerated <= 1:\n",
    "                max_corr_tolerated = 5\n",
    "        else:\n",
    "            type_corr = 'r_squared'\n",
    "            if max_corr_tolerated > 1:\n",
    "                max_corr_tolerated = 0.8\n",
    "        \n",
    "        ### Turning the var cols_to_avoid into a list in case user inputs a single columns in a string format instead of in a list\n",
    "        if cols_to_avoid != None:\n",
    "            if type(cols_to_avoid) == str:\n",
    "                cols_to_avoid = [cols_to_avoid]\n",
    "\n",
    "        ### Creating df which is a copy of the parameter dataframe so that the original dataframe is not changed\n",
    "        df = dataframe.copy()\n",
    "        ### dropping cols_to_avoid from df\n",
    "        if cols_to_avoid != None:\n",
    "            df2 = df.drop(cols_to_avoid,axis = 1)\n",
    "        else:\n",
    "            df2 = df.copy()\n",
    "\n",
    "        ### Setting cols_to_use as list of all the columns in df\n",
    "        if cols_to_use == None:\n",
    "            cols_to_use = list(df2.columns)\n",
    "            if cols_to_avoid != None:\n",
    "                cols_to_use = [col for col in cols_to_use if col not in cols_to_avoid]\n",
    "            \n",
    "        else:\n",
    "            ### Turning the var cols_to_use into a list in case user inputs a single columns in a string format instead of in a list\n",
    "            if type(cols_to_use) == str:\n",
    "                cols_to_use = [cols_to_use]\n",
    "            \n",
    "            if cols_to_avoid != None:\n",
    "                cols_to_use = [col for col in cols_to_use if col not in cols_to_avoid]\n",
    "                \n",
    "        if num_corr_cols_tolerated == None:\n",
    "            num_corr_cols_tolerated = int(np.ceil(df2.shape[1]/10))\n",
    "        \n",
    "        ### Creating a matrix, high_corr_cols_matrix, which contains a list for each feature which contains the feature name as the \n",
    "        ### first element and a list of all highly correlated features as the second element\n",
    "        high_corr_cols_count_matrix = []\n",
    "        high_corr_cols_list_dict = {}\n",
    "        \n",
    "        for col in cols_to_use:\n",
    "            if type_corr == 'vif':\n",
    "                corr_df = (1 / (1-df2[cols_to_use].corr()**2))[[col]].drop(col,axis = 0)\n",
    "            elif type_corr == 'r_squared':\n",
    "                corr_df = (df2[cols_to_use].corr()**2)[[col]].drop(col,axis = 0)\n",
    "            high_corr_cols_count_matrix.append([col,len(list(corr_df[corr_df[col] > max_corr_tolerated].index))])\n",
    "            ### Creating a dictionary which contains each feature as a key and a list containing all feautures highly correlated to that feature as the value\n",
    "            high_corr_cols_list_dict[col] = list(corr_df[corr_df[col] > max_corr_tolerated].index)\n",
    "            \n",
    "        high_corr_cols_df = pd.DataFrame(high_corr_cols_count_matrix,columns = ['percent_col','num_highly_corr_cols'])\n",
    "        ### Creates a list of features which have high correlation with only a few other features\n",
    "        list_low_corr_cols = list(high_corr_cols_df[high_corr_cols_df['num_highly_corr_cols'] <= num_corr_cols_tolerated]['percent_col'].values)\n",
    "        \n",
    "        # IMPORTANT: Eliminating only one of the features if a feature has only one other highly correlated feature\n",
    "        for col in list(high_corr_cols_df[high_corr_cols_df['num_highly_corr_cols'] == 1].index):\n",
    "            if col in list_low_corr_cols:\n",
    "                list_low_corr_cols.remove(high_corr_cols_list_dict[col][0])\n",
    "        \n",
    "        ### Creating a filterd df, filt_df, which is a df of all the lowly correlated features\n",
    "        if cols_to_avoid != None:\n",
    "            filt_df = df[list_low_corr_cols].merge(df[cols_to_avoid],how = 'inner',left_index = True,right_index = True)\n",
    "        else:\n",
    "            filt_df = df[list_low_corr_cols]\n",
    "        return filt_df,list_low_corr_cols\n",
    "    \n",
    "    ### First iteration of remove_high_corr_cols_one_iter followed by multiple iterations (accoring to var num_iterations) and\n",
    "    ### a final iteration where num_corr_cols_tolerated = 1\n",
    "    dataframe,cols_to_use = remove_high_corr_cols_one_iter()\n",
    "    if num_iterations > 1 and num_corr_cols_tolerated > num_iterations:\n",
    "        num_corr_cols_tolerated -= np.ceil(num_corr_cols_tolerated/num_iterations)\n",
    "        for i in range(num_iterations-2):\n",
    "            dataframe,cols_to_use = remove_high_corr_cols_one_iter()\n",
    "    \n",
    "    dataframe,cols_to_use = remove_high_corr_cols_one_iter(num_corr_cols_tolerated = 0)\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_cat(df, cat_cols=None,output_col=None,reset_index = False):\n",
    "    \"\"\"\n",
    "    This function converts all categorical columns into numerical boolean columns.\n",
    "    \n",
    "    There are 3 parameters: df, cat_cols, and output_var. \n",
    "    \n",
    "    1. df is the dataframe which needs to have categorical variables converted to numerical variables\n",
    "    2. cat_cols needs to be a list that contains the names of all categorical columns that need to be converted.\n",
    "    3. output_var is the name of the output or response variable.  It is set to 'Output' as default.\"\"\"\n",
    "    \n",
    "    if cat_cols == None:\n",
    "        cat_cols = [col for col in df.columns if df[col].dtype in ['O']]\n",
    "    \n",
    "    if output_col != None and output_col in cat_cols:\n",
    "        cat_cols.remove(output_col)\n",
    "    cat_cols = list(set(cat_cols))\n",
    "    \n",
    "    df_out = df.copy()\n",
    "    if reset_index == True:\n",
    "        df_out.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        dummy_col = pd.get_dummies(df_out[col],prefix = col+'_',drop_first = True)\n",
    "        df_out.drop(col,axis=1, inplace = True)\n",
    "\n",
    "        df_out = df_out.join(dummy_col)\n",
    "\n",
    "    if output_col != None:\n",
    "        loc_df_out = list(df_out.columns).index(output_col)\n",
    "        df_out = df_out[list(df_out.columns[:loc_df_out]) + list(df_out.columns[loc_df_out + 1:]) + list(df_out.columns[loc_df_out:loc_df_out + 1])]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_dummy_variable_columns(data,dummy_var_cols,name_agg_col,remove_common_word = False,common_word = None):\n",
    "    \"\"\"\n",
    "    This function combines the information stored in dummy variables into one column. It does the opposite function of the \n",
    "    convert_cat function and returns a dataframe with the newly added aggreate categorical variable column.\n",
    "    Parameters:\n",
    "    1. data: Pandas DataFrame\n",
    "    \n",
    "    2. dummy_var_cols: a list of dummy variables column names that are related to common category. \n",
    "       NOTE: The summation by row for each row across all columns in this list have to equal 1.\n",
    "       \n",
    "    3. name_agg_col: This is the name you want to give for the aggregate column.\n",
    "    \n",
    "    4. remove_common_word: This is a boolean variable which if set to True, means that a common word will be eliminated from the \n",
    "       final values in the name_agg_col column of resultant dataframe. Default: False\n",
    "       \n",
    "    5. common_word: this variable is the common_word that occurs in each of the dummy variable column names. \n",
    "       ex: common_word = education_ if dummy_var_cols = ['education_None','education_Bachelors','education_PhD']. Default: None\"\"\"\n",
    "    \n",
    "    df = data.copy()\n",
    "    dict_col_inds = {}\n",
    "    for ind in range(len(dummy_var_cols)):\n",
    "        if remove_common_word == False:\n",
    "            dict_col_inds[ind] = dummy_var_cols[ind]\n",
    "        else:\n",
    "            if common_word != None:\n",
    "                try:\n",
    "                    if dummy_var_cols[ind].split(common_word)[0] == '':\n",
    "                        dict_col_inds[ind] = dummy_var_cols[ind].split(common_word)[-1]\n",
    "                    else:\n",
    "                        dict_col_inds[ind] = dummy_var_cols[ind].split(common_word)[0]\n",
    "                except:\n",
    "                    print(\"Wrong common_word. Please choose a correct common_word for dummy variable columns which have a \\\n",
    "                          common word in their name in the same location (in the beginning or end of column name)\")\n",
    "            \n",
    "    def combine_columns(cols):\n",
    "        value_ind = list(cols).index(1)\n",
    "        return dict_col_inds[value_ind]\n",
    "    \n",
    "    df[name_agg_col] = df[dummy_var_cols].apply(combine_columns,axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(df,cols_to_ignore = None,target_col = None):\n",
    "    if target_col != None and cols_to_ignore!= None:\n",
    "        if target_col not in cols_to_ignore:\n",
    "            cols_to_ignore.append(target_col)\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes in a dataframe as a parameter and returns the same dataframe with all the features\n",
    "    normalized between 0 and 1 using rescaling (min-max normalization)\n",
    "    Defaults:\n",
    "    cols_to_ignore = None\n",
    "    target_col = None\n",
    "    \"\"\"\n",
    "    \n",
    "    if cols_to_ignore == None:\n",
    "        cols_to_ignore = []\n",
    "        if target_col != None and target_col not in cols_to_ignore:\n",
    "            cols_to_ignore.append(target_col)\n",
    "        \n",
    "    for col in list(df.columns):\n",
    "        if col not in cols_to_ignore and df[col].dtype.name not in ('int32','int64','float32','float64'):\n",
    "            cols_to_ignore.append(col)\n",
    "            \n",
    "    l_min = []\n",
    "    l_max = []\n",
    "    desc = df.describe()\n",
    "    if cols_to_ignore == None:\n",
    "        for col in df.columns:\n",
    "            l_min.append(desc[col]['min'])\n",
    "            l_max.append(desc[col]['max'])\n",
    "        \n",
    "        t_min = list(zip(df.columns, l_min))\n",
    "        t_max = list(zip(df.columns, l_max))\n",
    " \n",
    "\n",
    "    else:\n",
    "        for col in df.drop(cols_to_ignore,axis = 1).columns:\n",
    "            l_min.append(desc[col]['min'])\n",
    "            l_max.append(desc[col]['max'])\n",
    "\n",
    "        t_min = list(zip(df.drop(cols_to_ignore,axis = 1).columns, l_min))\n",
    "        t_max = list(zip(df.drop(cols_to_ignore,axis = 1).columns, l_max))\n",
    "    \n",
    "   \n",
    "    d_min = {}\n",
    "    for col,val in t_min:\n",
    "        d_min[col]=val\n",
    "    \n",
    "    d_max = {}\n",
    "    for col,val in t_max:\n",
    "        d_max[col]=val\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    for key in d_min.keys():\n",
    "        df_copy[key] = df_copy[key].apply(lambda x: (x - d_min[key])/ (d_max[key] - d_min[key]))\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_df(df,test_size = 0.3,df_to_return = 'df_train'):\n",
    "    \"\"\"\n",
    "    This function takes in a Pandas DataFrame and returns a \n",
    "    dataframe that is a subset of that Pandas DataFrame.\n",
    "    \n",
    "    There are 3 parameters: df, test_size, and df_to_return\n",
    "    \n",
    "    df needs to be a Pandas DataFrame and is the superset dataframe to be divided.\n",
    "    test_size is the proportion of the dataframe you want to be the testing dataset.\n",
    "    test_size is set to 0.3 by default.\n",
    "    df_to_return needs to specified as either 'df_train' or df_test' \n",
    "    to return the correct subset dataframe. df_to_return is set to 'df_train' by default\n",
    "    \"\"\"\n",
    "    split_num = int(df.count()[0] * (1-test_size) // 1)\n",
    "    df_train = df.iloc[:split_num,:]\n",
    "    df_test = df.iloc[split_num:,:]\n",
    "    if df_to_return in ['df_train','train']:\n",
    "        return df_train\n",
    "    elif df_to_return in ['df_test','test']:\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffled_split_dfs(df,test_size = 0.3,reset_index = True):\n",
    "    \"\"\"\n",
    "    This function takes in a Pandas DataFrame and returns a list of 2\n",
    "    dataframes.  The first dataframe is the train and the second is the test df.\n",
    "    \n",
    "    There are 3 parameters: df and test_size\n",
    "    \n",
    "    1. df needs to be a Pandas DataFrame and is the superset dataframe to be divided.\n",
    "    2. test_size is the proportion of the dataframe you want to be the testing dataset.\n",
    "    test_size is set to 0.3 by default\n",
    "    3. reset_index determines whether the index of the resultant dataframes should be reset. Default is True\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    if reset_index == True:\n",
    "        df_copy = df_copy.sample(frac = 1).reset_index(drop = True)\n",
    "    else:\n",
    "        df_copy = df_copy.sample(frac = 1)       \n",
    "    split_num = int(df_copy.count()[0] * (1-test_size) //1)\n",
    "    df_train = df_copy.iloc[:split_num,:]\n",
    "    df_test = df_copy.iloc[split_num:,:]\n",
    "    return ([df_train,df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_accuracy(df = None,pred_df = None, test_df = None, algo = 'lin',target_class = None):\n",
    "    import numpy as np\n",
    "    \"\"\"\n",
    "    This function takes in a pandas DataFrame and returns the accuracy of the model\n",
    "    \n",
    "    There are 5 parameters: df and algo\n",
    "    \n",
    "    1. df needs to be a Pandas DataFrame and algo is the algorithm used.\n",
    "    2. pred_df is the prediction dataframe used for the knn algorithm\n",
    "    3. test_df is the test dataframe used for the knn algorithm\n",
    "    4. algo is set to 'lin' by default but can also be specified as 'log' or 'knn'\n",
    "    5. target_class is the output_variable\n",
    "    \"\"\"\n",
    "    \n",
    "    if algo == 'lin':\n",
    "        df_out = df.copy()\n",
    "        df_out['error'] = df.iloc[:,-2] - df.iloc[:,-1]\n",
    "        \n",
    "        ME = df_out['error'].mean()\n",
    "        MAE = np.abs(df_out['error']).mean()\n",
    "        MSE = (df_out['error'] ** 2).mean()\n",
    "        RMSE = (sum(df_out['error']**2)/df_out.count()[0]+1) ** 0.5\n",
    "        SSres = (df_out['error']**2).sum()\n",
    "        SStot = ((df.iloc[:,-2] - df.iloc[:,-2].mean())**2).sum()\n",
    "        r_sqd = 1 - (SSres / SStot)\n",
    "        \n",
    "        return {'ME':ME,'MSE':MSE,'MAE':MAE,'RMSE':RMSE,'r_sqd' : r_sqd}\n",
    "    \n",
    "    elif algo == 'log':\n",
    "        from sklearn.metrics import confusion_matrix,classification_report\n",
    "        if target_class == None:\n",
    "            target_class = df.columns[-4]\n",
    "        print(confusion_matrix(df[target_class].values,df['Crisp'].values))\n",
    "        print(classification_report(df[target_class].values,df['Crisp'].values))\n",
    "    \n",
    "        return sum(df['Correct?']/df.count()[0])\n",
    "    \n",
    "    elif algo == 'knn':\n",
    "        pred = pred_df[[target_class]]\n",
    "        test = test_df[[target_class]]\n",
    "        pred[target_class] = pred[target_class].apply(lambda val: int(round(val)))\n",
    "        test[target_class] = test[target_class].apply(lambda val: int(round(val)))\n",
    "                                                      \n",
    "        if pred_df[target_class].nunique() in [1,2]:\n",
    "            from sklearn.metrics import confusion_matrix,classification_report\n",
    "            print(confusion_matrix(test,pred))\n",
    "            print(classification_report(test,pred))\n",
    "        #return sum(pred[target_class] == test[target_class])\n",
    "        print(\"Accuracy = %s percent\" %  (100 * sum(pred[target_class] == test[target_class]) / pred.count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def barplot_absolute(dataset,dep_col,ind_col,hue_col = None, is_aggregated_bool = False, aggregate_func = None, magnitude = None,ind_cols_order = None, plot_palette = 'mako', plt_figsize_x = 22, plt_figsize_y = 14, ymin = 0,y_max_multiplier = 1.3,num_decimals = 1, annotate_percentages = False, perc_annot_height_mult = 0.05, perc_annot_num_decimals = 1, plot_title_text = None, plot_title_fsize = 30, xlabel_text = None, xlabel_fsize = 22, ylabel_text = None, ylabel_fsize = 22, x_tick_fsize = 15, y_tick_fsize = 18, xtick_rotation = 65, ytick_rotation = 0, annot_fsize = 12, legend_fsize = 15, hlinewidth = 1, hlinecolor1 = 'black', hlinecolor2 = 'black'):\n",
    "    plt_df = dataset.copy()\n",
    "    overall_average = plt_df[dep_col].mean()\n",
    "    ### Need to aggregate data if it is not already so that there are not vertical lines in the barplot columns\n",
    "    if is_aggregated_bool == False:\n",
    "        if aggregate_func.lower() in ['sum','add','addition','summation']:\n",
    "            plt_df = plt_df.groupby(ind_col).sum().reset_index()\n",
    "        elif aggregate_func.lower() in ['count']:\n",
    "            plt_df = plt_df.groupby(ind_col).count().reset_index()\n",
    "        elif aggregate_func.lower() in ['std','standard deviation','stdev']:\n",
    "            plt_df = plt_df.groupby(ind_col).std().reset_index()\n",
    "        else:\n",
    "            plt_df = plt_df.groupby(ind_col).mean().reset_index()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if magnitude != None:\n",
    "        if magnitude.lower() in ['single','ones','single digit']:\n",
    "            magnitude = 1\n",
    "        elif magnitude.lower() in ['thousand','thousands']:\n",
    "            magnitude = 1000\n",
    "        elif magnitude.lower() in ['million','millions']:\n",
    "            magnitude = 1000000\n",
    "            \n",
    "    else:\n",
    "        if plt_df[dep_col].max() < 1000:\n",
    "            magnitude = 1\n",
    "        elif plt_df[dep_col].max() < 1000000 and plt_df[dep_col].max() >= 1000:\n",
    "            magnitude = 1000\n",
    "        elif plt_df[dep_col].max() < 1000000000 and plt_df[dep_col].max() >= 1000000:\n",
    "            magnitude = 1000000\n",
    "    mag_dict = {1:'', 1000:' in Thousands', 1000000:' in Millions'}\n",
    "    # Defining the plot size\n",
    "    plt.figure(figsize=(plt_figsize_x, plt_figsize_y))\n",
    "    \n",
    "    # Defining the values for x-axis, y-axis\n",
    "    # and from which dataframe the values are to be picked\n",
    "    \n",
    "    ### Changing the values of the dependent column based on magnitude variable so large values can be more easily readable.\n",
    "    plt_df[dep_col] = (plt_df[dep_col]/magnitude).round(decimals = 6)\n",
    "    if hue_col == None:\n",
    "        if ind_cols_order == None:\n",
    "            plots = sns.barplot(palette = plot_palette,x=ind_col, y=dep_col, data = plt_df)\n",
    "        else:\n",
    "            plots = sns.barplot(palette = plot_palette,x=ind_col, y=dep_col, data = plt_df, order = ind_cols_order)\n",
    "    else:\n",
    "        # DONT RUN: plt_df = plt_df.merge(plt_df[[hue_col]],how = 'inner',left_index = True,right_index = True).reset_index()\n",
    "        if ind_cols_order == None:\n",
    "            plots = sns.barplot(palette = plot_palette,x=ind_col, y=dep_col, data = plt_df,hue = hue_col,dodge = False)\n",
    "        else:\n",
    "            plots = sns.barplot(palette = plot_palette,x=ind_col, y=dep_col, data = plt_df,hue = hue_col,dodge = False, order = ind_cols_order)\n",
    "\n",
    "    # Iterrating over the bars one-by-one\n",
    "    for bar in plots.patches:\n",
    "\n",
    "      # Using Matplotlib's annotate function and passing the coordinates where the annotation shall be done\n",
    "      # x-coordinate: bar.get_x() + bar.get_width() / 2\n",
    "      # y-coordinate: bar.get_height()\n",
    "      # free space to be left to make graph pleasing: (0, 8)\n",
    "      # ha and va stand for the horizontal and vertical alignment\n",
    "        plots.annotate(format(bar.get_height(), '.%sf' % num_decimals),\n",
    "                       (bar.get_x() + bar.get_width() / 2,\n",
    "                        bar.get_height()), ha='center', va='center',\n",
    "                       size= annot_fsize, xytext=(0, 8),\n",
    "                       textcoords='offset points')\n",
    "                       \n",
    "        if annotate_percentages == True:\n",
    "            plots.annotate(\"(%s%%)\" % (format(100*bar.get_height()/plt_df.groupby(ind_col).mean()[dep_col].sum(), '.%sf' % perc_annot_num_decimals)),\n",
    "                           (bar.get_x() + bar.get_width() / 2,\n",
    "                            bar.get_height() + perc_annot_height_mult*plt_df[dep_col].max()), ha='center', va='center',\n",
    "                           size= annot_fsize, xytext=(0, 8),\n",
    "                           textcoords='offset points')\n",
    "\n",
    "    # Setting the label for x-axis\n",
    "    if xlabel_text == None:\n",
    "        plt.xlabel(\"%s\" % (ind_col[0].upper()+ind_col[1:]), size= xlabel_fsize)\n",
    "    else:\n",
    "        plt.xlabel(xlabel_text, size = xlabel_fsize)\n",
    "    plt.xticks(rotation = xtick_rotation,size = x_tick_fsize)\n",
    "\n",
    "    # Setting the label for y-axis\n",
    "    if ylabel_text == None:\n",
    "        plt.ylabel(\"%s%s\" % (' '.join([word[0].upper() + word[1:] for word in dep_col.split('_')]),mag_dict[magnitude]), \\\n",
    "                   size= ylabel_fsize)\n",
    "    else:\n",
    "        plt.ylabel(\"%s%s\" % (ylabel_text, mag_dict[magnitude]), size = ylabel_fsize)\n",
    "    plt.yticks(rotation = ytick_rotation, size = y_tick_fsize)\n",
    "    plt.ylim(ymin,plt_df[dep_col].max()*y_max_multiplier)\n",
    "    \n",
    "    ### Creating horizontal lines, one with one with overall average of all data points separately, and one with the average giving each category's average equal weightage\n",
    "    ind_col_text_clean = ' '.join(ind_col.split('_'))\n",
    "    \n",
    "    plt.axhline(y = overall_average, linewidth = hlinewidth, color = hlinecolor1,label = \"Overall Average (%s)\" % \\\n",
    "                (format((overall_average),'.%sf' % num_decimals)))\n",
    "    \n",
    "    plt.axhline(y = plt_df[dep_col].mean(), linewidth = hlinewidth, color = hlinecolor2, linestyle = '--',label = \"Average of %s Categories (%s)\" % \\\n",
    "                (ind_col_text_clean[0].upper()+ind_col_text_clean[1:], format((plt_df[dep_col].mean()),'.%sf' % num_decimals)))\n",
    "    \n",
    "    \n",
    "    plt.legend(loc = 'upper left',fontsize = legend_fsize)\n",
    "    # Setting the title for the graph\n",
    "    if plot_title_text == None:\n",
    "        plt.title(\"%s by %s\" % (' '.join([word[0].upper() + word[1:] for word in dep_col.split('_')]),\\\n",
    "                                (ind_col[0].upper() + ind_col[1:])), plot_title_fsize)\n",
    "    else:\n",
    "        plt.title(plot_title_text, size = plot_title_fsize)\n",
    "    plt.tight_layout()\n",
    "    # Finally showing the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def barplot_percent(dataset,dep_col,ind_col,total_col,hue_col = None):\n",
    "    # Defining the plot size\n",
    "    plt_df = dataset.copy()\n",
    "    plt.figure(figsize=(26, 14))\n",
    "    plt_df = plt_df.reset_index()\n",
    "\n",
    "    # Defining the values for x-axis, y-axis\n",
    "    # and from which dataframe the values are to be picked\n",
    "    if hue_col == None:\n",
    "        plots = sns.barplot(data = plt_df,x=ind_col, y=dep_col,palette = 'mako')\n",
    "    else:\n",
    "        plots = sns.barplot(data = plt_df,x=ind_col, y=dep_col,hue = hue_col,palette = 'mako',dodge = False)\n",
    "    \n",
    "    # Iterrating over the bars one-by-one\n",
    "    for bar in plots.patches:\n",
    "        plots.annotate(format(bar.get_height(), '.2f'),\n",
    "                       (bar.get_x() + bar.get_width() / 2,\n",
    "                        bar.get_height()), ha='center', va='center',\n",
    "                       size=12, xytext=(0, 8),\n",
    "                       textcoords='offset points')\n",
    "    \n",
    "    # Setting the label for x-axis\n",
    "    plt.xlabel(\"%s\" % (ind_col[0].upper() + ind_col[1:]), size=24)\n",
    "    plt.xticks(rotation = 65,size = 12)\n",
    "\n",
    "    # Setting the label for y-axis\n",
    "    plt.ylabel(' '.join([word[0].upper() + word[1:] for word in dep_col.split('_')]), size=24)\n",
    "    plt.yticks(size = 18)\n",
    "    plt.ylim(0,plt_df[dep_col].max()*1.3)\n",
    "\n",
    "    plt.axhline(y = plt_df[dep_col].mean(),color = 'black',label = \"%s Average\\n(%s percent)\" % (ind_col[0].upper() + ind_col[1:],format((plt_df[dep_col].mean()), '.2f')))\n",
    "    plt.axhline(y = plt_df[[total_col,dep_col]].apply(lambda cols: (cols[0]*cols[1])/plt_df[total_col].sum(),axis = 1)\\\n",
    "                .sum(),color = 'blue',label = \"Overall Average\\n(%s percent)\" % format(plt_df[[total_col,dep_col]].apply(lambda cols: (cols[0]*cols[1])/plt_df[total_col].sum(),axis = 1).sum(), '.2f'))\n",
    "    plt.legend(loc = 'upper left',fontsize = 15)\n",
    "\n",
    "\n",
    "    # Setting the title for the graph\n",
    "    plt.title(\"%s by %s\" % ((' '.join([word[0].upper() + word[1:] for word in dep_col.split('_')])),ind_col[0].upper() + ind_col[1:]),size = 30)\n",
    "    plt.tight_layout()\n",
    "    # Finally showing the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pie_plot(dataset, ind_col, plot_title, legend_title, dep_col = None, agg_func_name = 'count', annotate_percentages = False, is_dollar_value = False, plt_figsize_x = 8, plt_figsize_y = 8, plot_title_fsize = 18, legend_text_fsize = 15, legend_title_fsize = 15, annot_fsize = 20, annot_text_color = 'w',plot_colors_list = ['xkcd:aquamarine','darkblue','xkcd:navy blue','xkcd:blue','teal','xkcd:lightblue','lightblue','xkcd:turquoise','xkcd:azure','xkcd:teal']):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(aspect=\"equal\"))\n",
    "    df = dataset.copy()\n",
    "    if agg_func_name.lower() in ['mean','avg','average']:\n",
    "        df = df.groupby(ind_col).mean()[dep_col]\n",
    "\n",
    "    elif agg_func_name.lower() in ['sum','summation','addition']:\n",
    "        df = df.groupby(ind_col).sum()[dep_col]\n",
    "\n",
    "    else:\n",
    "        df = df.groupby(ind_col).count()[df.columns[0]]\n",
    "\n",
    "    def return_annotation_with_perc(pct, allvals):\n",
    "        absolute = int(np.round(pct/100.*np.sum(allvals)))\n",
    "        if is_dollar_value == False:\n",
    "            return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n",
    "        else:\n",
    "            absolute = (\"${:,}\".format(absolute))\n",
    "            return \"{:.1f}%\\n({:s})\".format(pct, absolute)\n",
    "        \n",
    "    def return_annotation(pct, allvals):\n",
    "        absolute = int(np.round(pct/100.*np.sum(allvals)))\n",
    "        if is_dollar_value == False:\n",
    "            return \"({:d})\".format(absolute)\n",
    "        else:\n",
    "            absolute = (\"${:,}\".format(absolute))\n",
    "            return \"({:s})\".format(absolute)    \n",
    "\n",
    "    if annotate_percentages == True:\n",
    "        wedges, texts, autotexts = ax.pie(df, autopct=lambda pct: return_annotation_with_perc(pct, df), textprops=dict(color=annot_text_color),\\\n",
    "                                          colors = plot_colors_list)\n",
    "        \n",
    "    else:\n",
    "        wedges, texts, autotexts = ax.pie(df, autopct=lambda pct: return_annotation(pct, df), textprops=dict(color=annot_text_color),\\\n",
    "                                          colors = plot_colors_list)\n",
    "        \n",
    "    pie_legend = ax.legend(wedges, df.index,\n",
    "              title=legend_title,\n",
    "              loc=\"best\",\n",
    "              bbox_to_anchor=(1, 0, 0.5, 1),fontsize = legend_text_fsize)\n",
    "\n",
    "    plt.setp(pie_legend.get_title(),fontsize = legend_title_fsize)\n",
    "    plt.setp(autotexts, size=annot_fsize, weight=\"bold\")\n",
    "\n",
    "    ax.set_title(plot_title,fontsize = plot_title_fsize,loc = 'center')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear Regression and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stoch_grad_desc(dataset,output_col,cols_to_ignore = None,alpha = 0.1,epoch = 10,algo = 'lin',initial_coeffs = 1, bias_coeff = None):\n",
    "    \n",
    "    # Importing time module to time program\n",
    "    from time import time\n",
    "    start_time = time()\n",
    "    \n",
    "    # For linear regression gradient descent\n",
    "    # coeffs = list((coeffs +  (alpha * feat_df.loc[row,:].transpose() * (dataset_1.loc[row,output_col] - pred)).transpose()).values)\n",
    "\n",
    "    # For logistic regression gradient descent\n",
    "    # coeffs = list((coeffs +  (alpha * feat_df.loc[row,:].transpose() * (dataset_1.loc[row,output_col] - pred)).transpose()).values)\n",
    "    \"\"\"\n",
    "    This function returns a list of the coefficients for the specified algorithm.  \n",
    "    Currently, this function only performs Linear and Logistic Regression.\n",
    "    \n",
    "    The 7 parameters are: dataset, output_col, cols_to_ignore, alpha, epoch, algo, and initial_coeffs\n",
    "    \n",
    "    IMPORTANT: DATAFRAME CANNOT HAVE NULL VALUES.  FEATURES SHOULD BE NORMALIZED.  INDEX VALUES SHOULD BE CONTINUOUS INTEGERS (EX 0, 1, 2). IF ANY OF THESE CONDITIONS ARE NOT MET, COEFFICIENTS WILL BE NULLS\n",
    "    \n",
    "    1. dataset needs to be a pandas DataFrame. No Default\n",
    "    2. output_col is the target column name as a string.  No Default\n",
    "    3. cols_to_ignore is a list of columns that the model will ignore including unwanted features and categorical features.\n",
    "       Default is None object\n",
    "    4. alpha is the alpha value used in stochiastic gradient descent.  Default is 0.1.\n",
    "    5. epoch is the number of iterations through each row in the dataset algorithm will perform.  Default is 10.\n",
    "    6. algo is the specific algorithm to be used.  algo is 'lin' by default for Linear Regression but can also be specified\n",
    "       as 'log' for Logistic Regression\n",
    "    7. initial_coeffs is the value user wants all the coeffs to be initialized to.  Default is 1.\n",
    "    8. bias_coeff is the y_intercept or the bias coefficient.  It is the value of y when all features have a value of 0.  It is set at None\n",
    "       meaning there is no set bias coefficient but it can be set to any floating point or integer number.\n",
    "    \"\"\"\n",
    "\n",
    "    loc_output_col = list(dataset.columns).index(output_col)\n",
    "    dataset = dataset[list(dataset.columns[:loc_output_col]) + list(dataset.columns[loc_output_col+1:]) + list(dataset.columns[loc_output_col:loc_output_col + 1])]\n",
    "    \n",
    "    from math import exp\n",
    "    count_rows = dataset.count()[1]\n",
    "\n",
    "    if cols_to_ignore != None:\n",
    "        dataset_1 = pd.DataFrame(pd.Series(np.ones(dataset.count()[0])),columns = ['X0']).join(dataset.drop(cols_to_ignore,axis = 1))\n",
    "    else:\n",
    "        dataset_1 = pd.DataFrame(pd.Series(np.ones(dataset.count()[0])),columns = ['X0']).join(dataset)\n",
    "    \n",
    "    feat_df = dataset_1.drop(output_col,axis = 1)\n",
    "    num_features = len(dataset_1.columns[0:-1])\n",
    "    coeffs = list(np.ones(num_features))\n",
    "    coeffs = [i * initial_coeffs for i in coeffs]\n",
    "    \n",
    "\n",
    "    df_columns = list(dataset_1.columns)\n",
    "    df_input_cols = df_columns\n",
    "    df_input_cols.remove(output_col)\n",
    "\n",
    "    for ep in range(epoch):\n",
    "        # Since this is stochastic gradient descent, the coefficients are updated after each row iteration\n",
    "        for row in range(count_rows):\n",
    "            y = dataset_1.loc[row,output_col]\n",
    "            \n",
    "            output_terms = []\n",
    "\n",
    "            for col in df_input_cols:\n",
    "                output_terms.append((coeffs[dataset_1.columns.get_loc(col)],dataset_1.loc[row,col]))\n",
    "\n",
    "            output_list = [(x*y) for (x,y) in output_terms]\n",
    "            \n",
    "            output = sum(output_list)\n",
    "            \n",
    "            if algo == 'lin':\n",
    "                pred = output\n",
    "                \n",
    "                # When number of features exceeds 26, the matrix implementaition is faster.  Otherwise the for loop \n",
    "                # implementation is faster\n",
    "                if bias_coeff != None and type(bias_coeff) in (int,float):\n",
    "                    coeffs[0] = bias_coeff\n",
    "                    \n",
    "                if num_features > 26:\n",
    "                    coeffs = list((coeffs +  (alpha * feat_df.loc[row,:].transpose() * (dataset_1.loc[row,output_col] - pred)).transpose()).values)\n",
    "                else:\n",
    "                    for i in range(len(coeffs)):\n",
    "                        coeffs[int(i)] += alpha * (dataset_1.loc[row,output_col] - pred) * dataset_1.iloc[row,int(i)]\n",
    "                        \n",
    "                if bias_coeff != None and type(bias_coeff) in (int,float):\n",
    "                    coeffs[0] = bias_coeff\n",
    "            elif algo == 'log':\n",
    "                pred = 1 / (1 + exp(-output))\n",
    "                \n",
    "                # When number of features exceeds 35, the matrix implementaition is faster.  Otherwise the for loop \n",
    "                # implementation is faster\n",
    "                \n",
    "                if num_features > 35:\n",
    "                    coeffs = list((coeffs + alpha * (y - pred) * pred * (1 - pred) * (feat_df.loc[row,:])).values)\n",
    "                else:\n",
    "                    for i in range(len(coeffs)):\n",
    "                        coeffs[i] = coeffs[i] + alpha * (y - pred) * pred * (1 - pred) * dataset_1.iloc[row,i]\n",
    "                        \n",
    "    ### This is to print how long the program took to run              \n",
    "    end_time = time()\n",
    "    run_time = end_time - start_time\n",
    "\n",
    "    mins = run_time // 60\n",
    "    secs = run_time % 60\n",
    "    hours = mins // 60\n",
    "    mins = mins % 60\n",
    "    \n",
    "    print(\"Program took %s hr(s), %s min(s), %s sec(s) to run\" % (hours,mins,secs))\n",
    "    \n",
    "    # Returns a list of all the coeffs (length should be number of features + 1 due to B0)\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(dataset, coeff_list, output_col,cols_to_ignore = None,algo = 'lin',log_reg_thresh = 0.5):\n",
    "    \"\"\"\n",
    "    This function takes in a pandas DataFrame and a list that contains \n",
    "    coefficients for the specified algorith used in the stoch_grad_desc function\n",
    "    and returns the same dataset (with the addition of the first column being 1s to\n",
    "    represent X0 in the regression formula) plus a new column at the end, 'Prediction'.\n",
    "    \n",
    "    There are 4 parameters: dataset, coeff_list, algo, and log_reg_thresh\n",
    "    \n",
    "    1. dataset is the dataframe to used to make predictions dataset needs to be a Pandas DataFrame\n",
    "    2. coeff_list should be the list that was the result of running the stoch_grad_desc function\n",
    "    3. algo is the specific algorithm to be used.  algo is set to 'lin' by default but can be set to 'log'\n",
    "    4. log_reg_thresh is the cut-off point for a positive prediction for the logistic regression model\"\"\"\n",
    "    \n",
    "    loc_output_col = list(dataset.columns).index(output_col)\n",
    "    dataset = dataset[list(dataset.columns[:loc_output_col]) + list(dataset.columns[loc_output_col+1:]) + list(dataset.columns[loc_output_col:loc_output_col + 1])]\n",
    "    dataset_index = dataset.index\n",
    "    \n",
    "    if type(cols_to_ignore) == list and cols_to_ignore != None:\n",
    "        df_ignored_cols = dataset.loc[:,cols_to_ignore]\n",
    "    elif type(cols_to_ignore) != list and cols_to_ignore != None:\n",
    "        df_ignored_cols = dataset.loc[:,[cols_to_ignore]]\n",
    "    if cols_to_ignore != None:\n",
    "        df_ignored_cols.reset_index(inplace = True)\n",
    "    \n",
    "    from math import exp\n",
    "    \n",
    "    dataset.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    if cols_to_ignore != None:\n",
    "        dataset_out = pd.DataFrame(pd.Series(np.ones(dataset.count()[0]))).join(dataset.drop(cols_to_ignore,axis = 1))\n",
    "    else:\n",
    "        dataset_out = pd.DataFrame(pd.Series(np.ones(dataset.count()[0]))).join(dataset)\n",
    "\n",
    "    \n",
    "    dataset_out.rename(mapper = {0:'X0'},axis = 1, inplace = True)\n",
    "    \n",
    "    coeffs = coeff_list\n",
    "    pred = []\n",
    "    \n",
    "    for row in range(dataset_out.count()[0]):\n",
    "        output_terms = []\n",
    "        for col in dataset_out.columns[0:-1]:\n",
    "            output_terms.append((coeffs[dataset_out.columns.get_loc(col)],dataset_out.loc[row,col]))\n",
    "        output_list = [x*y for (x,y) in output_terms]\n",
    "        \n",
    "        output = sum(output_list)\n",
    "        if algo == 'lin':\n",
    "            pred.append(output)\n",
    "        \n",
    "        elif algo == 'log':\n",
    "            pred.append(1/(1 + exp(-output)))\n",
    "    dataset_out = dataset_out.join(pd.DataFrame(pred))\n",
    "    dataset_out.rename(mapper = {0: 'Prediction'},axis = 1, inplace = True)\n",
    "    \n",
    "    if algo == 'log':\n",
    "        dataset_out['Crisp'] = dataset_out['Prediction'].apply(lambda predi: 1 if predi >= log_reg_thresh else 0)\n",
    "        dataset_out['Correct?'] = dataset_out.iloc[:,-3] == dataset_out['Crisp']\n",
    "    \n",
    "    if cols_to_ignore != None:\n",
    "        dataset_out = pd.concat([df_ignored_cols,dataset_out],axis = 1)\n",
    "    dataset_out.drop('X0',axis = 1,inplace = True)\n",
    "    \n",
    "    dataset_out.set_index(dataset_index,inplace = True)\n",
    "    return dataset_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn(df_train,new_pts_list,dep_col,k = 7,only_pred_df = True):\n",
    "    \"\"\"\n",
    "    This function performs a K Nearest Neighbors Algorithm with df_train being the labeled dataframe and new_pts_list being the \n",
    "    unlabeled dataframe.  dep_col is the target_variable name, k is the number of neighbors.  If only_pred_df is true,\n",
    "    only the predictions will be outputted as a dataframe.  If it is false, the entire dataframe inputted will be returned\n",
    "    with values for the predictions in the same dataframe\n",
    "    Defaults:\n",
    "    k = 7\n",
    "    only_pred_df = True\"\"\"\n",
    "    \n",
    "    from time import time\n",
    "    start_time = time()\n",
    "    import statistics\n",
    "    import numpy as np\n",
    "    \n",
    "    nrows_new_pts = new_pts_list.shape[0]\n",
    "    loc_dep_col = list(df_train.columns).index(dep_col)\n",
    "    df_in = df_train[list(df_train.columns)[0:loc_dep_col] + list(df_train.columns)[loc_dep_col + 1:] + list(df_train.columns)[loc_dep_col:loc_dep_col + 1]] \n",
    "    \n",
    "    if type(new_pts_list) == type(df_in):\n",
    "        l_list_new_pts = []\n",
    "        \n",
    "        for rn in range(new_pts_list.count()[0]):\n",
    "            l_list_new_pts.append(list(new_pts_list.iloc[rn,:]))\n",
    "        new_pts_list = l_list_new_pts\n",
    "\n",
    "    def knn_1pt(df,new_point,output_col = dep_col,k = k):\n",
    "        \n",
    "        df1 = df.copy()\n",
    "        new_pt_df = pd.DataFrame(data = [new_point + ['DK']],columns=df1.columns)\n",
    "        #return new_pt_df\n",
    "        df_out = df1.append(new_pt_df,ignore_index = True)\n",
    "        #return df_out\n",
    "\n",
    "        df2 = df_out.drop(output_col,axis=1)\n",
    "        #return df2\n",
    "        count_rows = df2.count()[0]\n",
    "        new_pt_ind = count_rows - 1\n",
    "        \n",
    "        dist_list = np.square((np.matrix(df2[:new_pt_ind]) - np.array(df2[new_pt_ind:]))).sum(axis = 1).transpose()[0].tolist()[0]\n",
    "        #return dist_list\n",
    "        enum_list = list(enumerate(dist_list))\n",
    "        enum_list.sort(key=lambda x:x[1])\n",
    "        #return enum_list\n",
    "        top_tup_list = enum_list[:k]\n",
    "        #return top_tup_list\n",
    "        closest_ind_list = [ind for (ind,dist) in enum_list[0:k]]\n",
    "        labels_list = list(df_out.loc[closest_ind_list,output_col].values)\n",
    "        #return labels_list\n",
    "        sorted_labels = sorted(labels_list,key = labels_list.count,reverse = True)\n",
    "        #return sorted_labels\n",
    "        if k == 1:\n",
    "            df_out.loc[new_pt_ind,output_col] = sorted_labels[0]\n",
    "        else:\n",
    "            try:\n",
    "                mode = statistics.mode(sorted_labels)\n",
    "            except:\n",
    "                mode = sorted_labels[0]\n",
    "        \n",
    "            finally:\n",
    "                df_out.loc[new_pt_ind,output_col] = mode\n",
    "        return df_out\n",
    "        \n",
    "    \n",
    "    for new_point in new_pts_list:\n",
    "        df_in =  knn_1pt(new_point = new_point,df=df_in).copy()\n",
    "    \n",
    "    end_time = time()\n",
    "    run_time = end_time - start_time\n",
    "\n",
    "    mins = run_time // 60\n",
    "    secs = run_time % 60\n",
    "    hours = mins // 60\n",
    "    mins = mins % 60\n",
    "    \n",
    "    print(\"Program took %s hr(s), %s min(s), %s sec(s) to run\" % (hours,mins,secs))    \n",
    "    \n",
    "    if only_pred_df == True:\n",
    "        return df_in[-nrows_new_pts:]\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def knn_1pt(df,new_point,output_col,k = 3):\n",
    "#     import statistics\n",
    "#     df1 = df.copy()\n",
    "#     new_pt_df = pd.DataFrame(data = [new_point + ['DK']],columns=df.columns)\n",
    "#     df_out = df1.append(new_pt_df,ignore_index = True)\n",
    "    \n",
    "#     df2 = df_out.drop(output_col,axis=1)\n",
    "#     count_rows = df2.count()[0]\n",
    "#     new_pt_ind = count_rows - 1\n",
    "#     df2['sum_sqrd_diffs'] = 0\n",
    "#     for row_num in range(0,count_rows):\n",
    "#         sum_sqrd_diffs = 0\n",
    "#         for col_num in range(0,len(df2.columns)):\n",
    "#             sum_sqrd_diffs += (df2.iloc[new_pt_ind,col_num] - df2.iloc[row_num,col_num])**2\n",
    "#         df_out.loc[row_num,'sum_sqrd_diffs'] = sum_sqrd_diffs \n",
    "       \n",
    "#     df_out.loc[new_pt_ind,output_col] = statistics.mode(df_out.iloc[:new_pt_ind,:].sort_values('sum_sqrd_diffs').head(k)[output_col])\n",
    "#     #return df_out.sort_values('sum_sqrd_diffs').head(10)\n",
    "#     return df_out\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def knn_old(df_train,new_pts_list,dep_col,k = 7,only_pred_df = True):\n",
    "#     from time import time\n",
    "#     start_time = time()\n",
    "#     import statistics\n",
    "    \n",
    "#     nrows_new_pts = new_pts_list.shape[0]\n",
    "#     loc_dep_col = list(df_train.columns).index(dep_col)\n",
    "#     df_in = df_train[list(df_train.columns)[0:loc_dep_col] + list(df_train.columns)[loc_dep_col + 1:] + list(df_train.columns)[loc_dep_col:loc_dep_col + 1]] \n",
    "    \n",
    "#     if type(new_pts_list) == type(df_in):\n",
    "#         l_list_new_pts = []\n",
    "        \n",
    "#         for rn in range(new_pts_list.count()[0]):\n",
    "#             l_list_new_pts.append(list(new_pts_list.iloc[rn,:]))\n",
    "#         new_pts_list = l_list_new_pts\n",
    "\n",
    "#     def knn_1pt(df,new_point,output_col = dep_col,k = k):\n",
    "        \n",
    "#         df1 = df.copy()\n",
    "#         new_pt_df = pd.DataFrame(data = [new_point + ['DK']],columns=df1.columns)\n",
    "#         df_out = df1.append(new_pt_df,ignore_index = True)\n",
    "\n",
    "#         df2 = df_out.drop(output_col,axis=1)\n",
    "#         count_rows = df2.count()[0]\n",
    "#         new_pt_ind = count_rows - 1\n",
    "#         df2['sum_sqrd_diffs'] = 0\n",
    "#         for row_num in range(0,count_rows):\n",
    "#             sum_sqrd_diffs = 0\n",
    "#             for col_num in range(0,len(df2.columns)):\n",
    "#                 sum_sqrd_diffs += (df2.iloc[new_pt_ind,col_num] - df2.iloc[row_num,col_num])**2\n",
    "#             df_out.loc[row_num,'sum_sqrd_diffs'] = sum_sqrd_diffs \n",
    "            \n",
    "#         if k == 1:\n",
    "#             df_out.loc[new_pt_ind,output_col] = statistics.mode(df_out.iloc[:new_pt_ind,:].sort_values('sum_sqrd_diffs').head(k)[output_col])\n",
    "#             df_in = df_out.drop('sum_sqrd_diffs',axis = 1).copy()\n",
    "#         else:\n",
    "#             try:\n",
    "#                 mode = statistics.mode(df_out.sort_values('sum_sqrd_diffs')[0:k][output_col])\n",
    "#             except:\n",
    "#                 closest = list(df_out.sort_values('sum_sqrd_diffs')[0:k][output_col].values)\n",
    "#                 copy = closest[:]\n",
    "#                 closest.sort(key = lambda x:copy.count(x))\n",
    "#                 closest.reverse()\n",
    "#                 mode = closest[0]\n",
    "#             df_out.loc[new_pt_ind,output_col] = mode\n",
    "#             df_in = df_out.drop('sum_sqrd_diffs',axis = 1).copy()\n",
    "#         return df_in\n",
    "     \n",
    "#     for np in new_pts_list:\n",
    "#         df_in =  knn_1pt(new_point = np,df=df_in).copy()\n",
    "    \n",
    "#     end_time = time()\n",
    "#     run_time = end_time - start_time\n",
    "\n",
    "#     mins = run_time // 60\n",
    "#     secs = run_time % 60\n",
    "#     hours = mins // 60\n",
    "#     mins = mins % 60\n",
    "    \n",
    "#     print(\"Program took %s hr(s), %s min(s), %s sec(s) to run\" % (hours,mins,secs))    \n",
    "    \n",
    "#     if only_pred_df == True:\n",
    "#         return df_in[-nrows_new_pts:]\n",
    "#     return df_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans(df,k=3,epoch = 1):\n",
    "    \"\"\"\n",
    "    This function performs a KMean Clustering algorithm to cluster on the input dataframe, df.  k is the number of clusters.\n",
    "    epoch is the number of iterations through the dataset the algorithm will perform.\n",
    "    Defaults:\n",
    "    k = 3\n",
    "    epoch = 1\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    def euclid_squared_distance(pt1,pt2):\n",
    "        return (pt1 - pt2)**2\n",
    "    \n",
    "    def select_k_points(dataframe = df,k = k):\n",
    "        centroid_dataframe = pd.DataFrame(columns = dataframe.columns)\n",
    "        centroid_indices = []\n",
    "        for i in range(k):\n",
    "            while True:\n",
    "                rand_ind = random.choice(list(dataframe.index))\n",
    "                if rand_ind not in centroid_indices:\n",
    "                    centroid_indices.append(rand_ind)\n",
    "                    break\n",
    "            centroid_dataframe = centroid_dataframe.append(pd.DataFrame(data = [list(dataframe.loc[rand_ind,:])],columns = dataframe.columns,index = [rand_ind]))\n",
    "            for row_num,ind in enumerate(list(centroid_dataframe.index)):\n",
    "                centroid_dataframe.loc[ind,'Cluster'] = int(row_num)\n",
    "        return centroid_dataframe\n",
    "    \n",
    "#     def kmeans_pp(dataframe = df,k = k):\n",
    "#         centroid_dataframe = pd.DataFrame(columns = dataframe.columns)\n",
    "#         centroid_indices = []\n",
    "#         rand_ind = random.choice(list(dataframe.index))\n",
    "#         centroid_indices.append(rand_ind)\n",
    "#         centroid_dataframe.loc[rand_ind,:] = dataframe.iloc[rand_ind,:]\n",
    "        \n",
    "#         for i in range(k-1):\n",
    "#             for row in dataframe.index:\n",
    "#                 sqd_diffs_df = pd.DataFrame(columns = dataframe.columns)\n",
    "#                 for cent_ind in centroid_indices:                \n",
    "#                     sqd_diffs_df.loc[cent_ind,:] = (centroid_dataframe.loc[cent_ind,:] - dataframe.loc[row,:]) ** 2\n",
    "#                     sqd_diffs_df['Sum_Squared_Diffs'] = sqd_diffs_df.sum(axis = 1)\n",
    "#                 min_sqd_diffs = sqd_diffs_df['Sum_Squared_Diffs'].min()\n",
    "#                 sqd_diffs_df=sqd_diffs_df[sqd_diffs_df['Sum_Squared_Diffs'] = 8]\n",
    "\n",
    "            \n",
    "    df['Cluster'] = np.nan\n",
    "    \n",
    "    centroid_df = select_k_points()\n",
    "\n",
    "    centroid_indices = list(centroid_df.index)\n",
    "    \n",
    "    \n",
    "\n",
    "    def one_iteration_k_means(dataframe = df,centroid_dataframe = centroid_df,cluster_col = 'Cluster'):\n",
    "        for row in dataframe.index:\n",
    "            sqd_diffs_df = pd.DataFrame(columns = centroid_dataframe.drop(cluster_col,axis = 1).columns)\n",
    "            for cent_ind in centroid_indices:\n",
    "                sqd_diffs_df.loc[cent_ind,:] = (centroid_dataframe.drop(cluster_col,axis = 1).loc[cent_ind,:] - dataframe.drop(cluster_col,axis = 1).loc[row,:]) ** 2 \n",
    "            sqd_diffs_df['Sum_Sqd_Diffs'] = sqd_diffs_df.sum(axis = 1)\n",
    "            for row_num,ind in enumerate(list(sqd_diffs_df.index)):\n",
    "                sqd_diffs_df.loc[ind,'Cluster'] = int(row_num)\n",
    "\n",
    "            smallest_ssd = sqd_diffs_df['Sum_Sqd_Diffs'].min()\n",
    "\n",
    "            smallest_ssd_df = sqd_diffs_df[sqd_diffs_df['Sum_Sqd_Diffs'] == smallest_ssd]\n",
    "\n",
    "            closest_centroid = pd.DataFrame(data = [smallest_ssd_df.loc[smallest_ssd_df.index[0],:].values],columns = list(sqd_diffs_df.columns),index = smallest_ssd_df.index)\n",
    "\n",
    "            closest_centroid_ind = list(closest_centroid.index)[0]\n",
    "\n",
    "            df.loc[row,cluster_col] = centroid_dataframe.loc[closest_centroid_ind,cluster_col]\n",
    "\n",
    "            for row_cent in list(centroid_dataframe.index):\n",
    "                \n",
    "                if row_cent == closest_centroid_ind:\n",
    "                    for col in list(centroid_dataframe.drop(cluster_col,axis = 1).columns):\n",
    "                        centroid_dataframe.loc[row_cent,col] = (centroid_dataframe.loc[row_cent,col] + df.loc[row,col])/2\n",
    "                        \n",
    "        return centroid_dataframe\n",
    "    \n",
    "    for iteration in range(epoch):\n",
    "        centroid_df = one_iteration_k_means() \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cart(df):\n",
    "    feat_1 = df.columns[0]\n",
    "    feat_2 = df.columns[1]\n",
    "    target = df.columns[-1]\n",
    "    def gini(feat,row,dataset = df):\n",
    "        df_gini = df.copy()\n",
    "        #return df_gini.loc[1,feat]\n",
    "\n",
    "        for i in range(df_gini.count()[0]):\n",
    "            if df_gini.loc[i,feat] < df_gini.loc[row,feat]:\n",
    "                df_gini.loc[i,'Group'] = 0\n",
    "            else:\n",
    "                df_gini.loc[i,'Group'] = 1\n",
    "        try:\n",
    "            count_left = df_gini['Group'].value_counts()[0]\n",
    "        except:\n",
    "            count_left = 0\n",
    "        try:\n",
    "            count_right = df_gini['Group'].value_counts()[1]\n",
    "        except:\n",
    "            count_right = 0\n",
    "        \n",
    "        count_0_left = df_gini[(df_gini[target] == df_gini['Group']) & (df_gini[target] == 0)].count()[0]\n",
    "        count_1_left = df_gini[(df_gini[target] != df_gini['Group']) & (df_gini[target] == 1)].count()[0]\n",
    "        count_0_right = df_gini[(df_gini[target] != df_gini['Group']) & (df_gini[target] == 0)].count()[0]\n",
    "        count_1_right = df_gini[(df_gini[target] == df_gini['Group']) & (df_gini[target] == 1)].count()[0]\n",
    "\n",
    "        g_split = ((count_0_left/count_left) * (1 - count_0_left/count_left))+ \\\n",
    "                   ((count_0_right/count_right) * (1 - count_0_right/count_right)) + \\\n",
    "                   ((count_1_left/count_left) * (1 - count_1_left/count_left)) + \\\n",
    "                   ((count_1_right/count_right) * (1 - count_1_right/count_right)) \n",
    "        return g_split\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i in range(0,df.count()[0]):\n",
    "        gini_split = gini(dataset = df,feat = feat_1,row = i)\n",
    "        l.append((i,feat_1,df.loc[i,feat_1],gini_split))\n",
    "        \n",
    "    for i in range(0,df.count()[0]):\n",
    "        gini_split = gini(dataset = df,feat = feat_2,row = i)\n",
    "        l.append((i,feat_2,df.loc[i,feat_2],gini_split))\n",
    "    l_gini = [i[-1] for i in l]\n",
    "    gini_min = min(l_gini)\n",
    "    l_final = list(filter(lambda x: gini_min == x[-1],l))\n",
    "    \n",
    "    return l_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn Palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seaborn_palettes = ['Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', \n",
    "                'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens','Greens_r', 'Greys', 'Greys_r', 'OrRd', \n",
    "                'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1',\n",
    "                'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu','PuBuGn', 'PuBuGn_r', \n",
    "                'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', \n",
    "                'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', \n",
    "                'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', \n",
    "                'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', \n",
    "                'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', \n",
    "                'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r',\n",
    "                'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r','gist_heat', 'gist_heat_r','gist_ncar', 'gist_ncar_r',\n",
    "                'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', \n",
    "                'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r',\n",
    "                'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', \n",
    "                'inferno_r', 'magma', 'magma_r', 'mako','mako_r', \n",
    "                'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r',\n",
    "                'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow','rainbow_r',\n",
    "                'rocket', 'rocket_r', 'seismic', 'seismic_r', 'spring', 'spring_r',\n",
    "                'summer', 'summer_r', 'tab10','tab10_r', 'tab20', 'tab20_r', 'tab20b',\n",
    "                'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'twilight',\n",
    "                'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available functions:\n",
      "\n",
      "\n",
      "create_random_df(model,nrows,ncols,nclasses,target_multiplier = 1),\n",
      "\n",
      "find_cols_to_ignore(df,thresh_cat = 0.02,cols_to_ignore = None),\n",
      "\n",
      "impute_cols(df,k=None,null_cols = 'auto_select',thresh_null_cat = 0.02,cols_to_ignore = None),\n",
      "\n",
      "num_unique_values_in_cols(df,cols = None),\n",
      "\n",
      "remove_outliers(df_in,outlier_cols = None,thresh_cat = 0.02,remove = True),\n",
      "\n",
      "def remove_high_corr_cols(dataframe,cols_to_use = None,cols_to_avoid = None,num_corr_cols_tolerated = None,\n",
      "\tmax_corr_tolerated = 0.8,num_iterations = 5,type_corr = 'r_squared'),\n",
      "\n",
      "convert_cat(df, cat_cols = None,output_col=None,reset_index = False),\n",
      "\n",
      "combine_dummy_variable_columns(data,dummy_var_cols,name_agg_col,remove_common_word = False,common_word = None),\n",
      "\n",
      "normalize(df,cols_to_ignore = None,target_col = None),\n",
      "\n",
      "split_df(df,test_size = 0.3,df_to_return = 'df_train'),\n",
      "\n",
      "shuffled_split_dfs(df,test_size = 0.3),\n",
      "\n",
      "check_accuracy(df = None,pred_df = None, test_df = None, algo = 'lin',target_class = None),\n",
      "\n",
      "barplot_absolute(dataset,dep_col,ind_col,hue_col = None, is_aggregated_bool = False, aggregate_func = None, magnitude = None,ind_cols_order = None, plot_palette = 'mako', plt_figsize_x = 22, plt_figsize_y = 14, ymin = 0,y_max_multiplier = 1.3,num_decimals = 1, annotate_percentages = False, perc_annot_height_mult = 0.05, perc_annot_num_decimals = 1, plot_title_text = None, plot_title_fsize = 30, xlabel_text = None, xlabel_fsize = 22, ylabel_text = None, ylabel_fsize = 22, x_tick_fsize = 15, y_tick_fsize = 18, xtick_rotation = 65, ytick_rotation = 0, annot_fsize = 12, legend_fsize = 15, hlinewidth = 1, hlinecolor1 = 'black', hlinecolor2 = 'black'),\n",
      "\n",
      "barplot_percent(dataset,dep_col,ind_col,total_col,hue_col = None),\n",
      "\n",
      "stoch_grad_desc(dataset,output_col,cols_to_ignore = None,alpha = 0.1,epoch = 10,algo = 'lin',initial_coeffs = 1, bias_coeff = None),\n",
      "\n",
      "make_predictions(dataset, coeff_list, output_col,cols_to_ignore = None,algo = 'lin',log_reg_thresh = 0.5),\n",
      "\n",
      "knn(df_train,new_pts_list,dep_col,k = 7,only_pred_df = True),\n",
      "\n",
      "kmeans(df,k=3,epoch = 1),\n",
      "\n",
      "cart(df)\n",
      "\n",
      "\n",
      "Additional Info:\n",
      "\n",
      "seaborn_palettes is a list of all seaborn palette names\n"
     ]
    }
   ],
   "source": [
    "print(\"Available functions:\\\n",
    "\\n\\n\\ncreate_random_df(model,nrows,ncols,nclasses,target_multiplier = 1),\\\n",
    "\\n\\nfind_cols_to_ignore(df,thresh_cat = 0.02,cols_to_ignore = None),\\\n",
    "\\n\\nimpute_cols(df,k=None,null_cols = 'auto_select',thresh_null_cat = 0.02,cols_to_ignore = None),\\\n",
    "\\n\\nnum_unique_values_in_cols(df,cols = None),\\\n",
    "\\n\\nremove_outliers(df_in,outlier_cols = None,thresh_cat = 0.02,remove = True),\\\n",
    "\\n\\ndef remove_high_corr_cols(dataframe,cols_to_use = None,cols_to_avoid = None,num_corr_cols_tolerated = None,\\\n",
    "\\n\\tmax_corr_tolerated = 0.8,num_iterations = 5,type_corr = 'r_squared'),\\\n",
    "\\n\\nconvert_cat(df, cat_cols = None,output_col=None,reset_index = False),\\\n",
    "\\n\\ncombine_dummy_variable_columns(data,dummy_var_cols,name_agg_col,remove_common_word = False,common_word = None),\\\n",
    "\\n\\nnormalize(df,cols_to_ignore = None,target_col = None),\\\n",
    "\\n\\nsplit_df(df,test_size = 0.3,df_to_return = 'df_train'),\\\n",
    "\\n\\nshuffled_split_dfs(df,test_size = 0.3),\\\n",
    "\\n\\ncheck_accuracy(df = None,pred_df = None, test_df = None, algo = 'lin',target_class = None),\\\n",
    "\\n\\nbarplot_absolute(dataset,dep_col,ind_col,hue_col = None, is_aggregated_bool = False, aggregate_func = None, magnitude = None,ind_cols_order = None, plot_palette = 'mako', plt_figsize_x = 22, plt_figsize_y = 14, ymin = 0,y_max_multiplier = 1.3,num_decimals = 1, annotate_percentages = False, perc_annot_height_mult = 0.05, perc_annot_num_decimals = 1, plot_title_text = None, plot_title_fsize = 30, xlabel_text = None, xlabel_fsize = 22, ylabel_text = None, ylabel_fsize = 22, x_tick_fsize = 15, y_tick_fsize = 18, xtick_rotation = 65, ytick_rotation = 0, annot_fsize = 12, legend_fsize = 15, hlinewidth = 1, hlinecolor1 = 'black', hlinecolor2 = 'black'),\\\n",
    "\\n\\nbarplot_percent(dataset,dep_col,ind_col,total_col,hue_col = None),\\\n",
    "\\n\\nstoch_grad_desc(dataset,output_col,cols_to_ignore = None,alpha = 0.1,epoch = 10,algo = 'lin',initial_coeffs = 1, bias_coeff = None),\\\n",
    "\\n\\nmake_predictions(dataset, coeff_list, output_col,cols_to_ignore = None,algo = 'lin',log_reg_thresh = 0.5),\\\n",
    "\\n\\nknn(df_train,new_pts_list,dep_col,k = 7,only_pred_df = True),\\\n",
    "\\n\\nkmeans(df,k=3,epoch = 1),\\\n",
    "\\n\\ncart(df)\\\n",
    "\\n\\n\\nAdditional Info:\\\n",
    "\\n\\nseaborn_palettes is a list of all seaborn palette names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
